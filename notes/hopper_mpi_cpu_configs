

The Python script is expected to spawn two threads:
srun --ntasks=1 --cpus-per-task=2 python my_py3_script.py

The srun will first allocate a total of 8 processes on 2 nodes
srun --nodes=2 --ntasks-per-node=4 --output=/scratch/%u/slurm-%N-%j.out ./my_mpi_program



To see total ram: grep MemTotal /proc/meminfo
To know about #-of cpus: lscpu

--ntasks=1 # Number of processes to launch. Most important parameter for parallal python. Max=300.
--cpus-per-task # must be <= #-of cores on a node, default=48. Max=64. Mostly not needed, but may use this when #-of threads are >48.
--nodes=2 # default=1. Mostly not needed.
--ntasks-per-node=4 # #-of processes to run in each node. Since --nodes not needed, we can skip this too.
--mem=16G # Total memory needed per task (units: K,M,G,T)
--mem-per-cpu # 

# 50 workers, 1 manager. This will be allocated into multiple nodes, since a node has max 48 cores, so 48 tasks can run on parallal. 
salloc --partition=normal --mem=16G --ntasks=51
# Contrarily, the following will run on 1 node.
salloc --partition=normal --mem=16G --ntasks=48


Some good resources: 
A template slurm: https://wiki.orc.gmu.edu/mkdocs/Template.slurm/
Running Multi-threaded jobs: https://wiki.orc.gmu.edu/mkdocs/How_to_run_Multi-threaded_job_on_ARGO/
Parallel python code: https://wiki.orc.gmu.edu/mkdocs/How_to_Write_Parallel_Python_Code/


salloc --partition=normal --ntasks=201 --cpus-per-task=2 --mem-per-cpu=4G
Note: I cannot give --node=1, bc 1 node has max=48 cpus (cores). But here I am allocating 101*4 cores.

if __name__ == '__main__':


# a python example below

import sys
sys.path.append("../variant_effect_analysis")

from mpi4py.futures import MPIPoolExecutor


# both of the examples run with the following setup:
# salloc --partition=normal --mem=1G --ntasks=3
## the default loaded module gnu9/9.3.0 and openmpi4/4.0.4 does not work with following mpirun command. so load as below which works.
# module load gnu10 #gnu10/10.3.0-ya
# module load openmpi # openmpi/4.1.2-4a
# mpirun -np $SLURM_NTASKS python -m mpi4py.futures mpi_practice/mpi_working_example.py

# if __name__ == '__main__':
#     executor = MPIPoolExecutor()
#     iterable = ((2, n) for n in range(32))
#     for result in executor.starmap(pow, iterable):
#        print(result)
#     executor.shutdown()


if __name__ == '__main__':
    executor = MPIPoolExecutor()
    for result in executor.map(pow, [2]*32, range(32)):
        print(result)

    executor.shutdown()
