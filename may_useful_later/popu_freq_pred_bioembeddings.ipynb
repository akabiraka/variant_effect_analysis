{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "home_dir = \"../../\"\n",
    "module_path = os.path.abspath(os.path.join(home_dir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# import sys\n",
    "# home_dir = \"\"\n",
    "# sys.path.append(\"../variant_effect_analysis\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import models.aa_common.pickle_utils as pickle_utils\n",
    "from models.aa_common.data_loader import get_population_freq_proteomic_SNVs, get_population_freq_proteomic_SNVs_fasta_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Log: Loading data ...\n",
      "raw data: (2882721, 8)\n",
      "Index(['prot_acc_version', 'pos', 'wt', 'mut', 'wt_population',\n",
      "       'mut_poulation', 'wt_freq', 'mt_freq'],\n",
      "      dtype='object')\n",
      "After combining common (18279), rare (29383) and sampled-singletons (47662), data: (95324, 8)\n",
      "\n",
      "Log: Loading combined fasta iterator ...\n"
     ]
    }
   ],
   "source": [
    "variants_df = get_population_freq_proteomic_SNVs(home_dir)\n",
    "fasta_iterator = get_population_freq_proteomic_SNVs_fasta_iterator(home_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Log: Model loading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at /home/akabir4/.cache/bio_embeddings/prottrans_bert_bfd/model_directory were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load model: 39.28939962387085 s\n",
      "(8, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLog: Model loading ...\")\n",
    "start = time.time()\n",
    "from models.bioembeddings_dallago.lm_heads.prottrans_lms_factory import load_prottrans_model\n",
    "model = load_prottrans_model(\"prottrans_bert_bfd\")\n",
    "tokenizer = model._tokenizer\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model: {end-start} s\")\n",
    "\n",
    "# an small example\n",
    "logits = model.embed(\"SEQVENCE\") # already converted to numpy array\n",
    "print(np.array(logits).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[UNK]': 1,\n",
       " '[CLS]': 2,\n",
       " '[SEP]': 3,\n",
       " '[MASK]': 4,\n",
       " 'L': 5,\n",
       " 'A': 6,\n",
       " 'G': 7,\n",
       " 'V': 8,\n",
       " 'E': 9,\n",
       " 'S': 10,\n",
       " 'I': 11,\n",
       " 'K': 12,\n",
       " 'R': 13,\n",
       " 'D': 14,\n",
       " 'T': 15,\n",
       " 'P': 16,\n",
       " 'N': 17,\n",
       " 'Q': 18,\n",
       " 'F': 19,\n",
       " 'Y': 20,\n",
       " 'M': 21,\n",
       " 'H': 22,\n",
       " 'C': 23,\n",
       " 'W': 24,\n",
       " 'X': 25,\n",
       " 'U': 26,\n",
       " 'B': 27,\n",
       " 'Z': 28,\n",
       " 'O': 29}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids(\"A\")) # this is not underscore(_), something else(‚ñÅ)\n",
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = home_dir+\"models/bioembeddings_dallago/\"\n",
    "logits_output_path = f\"{model_path}lm_outputs_{model.name}/\"\n",
    "model_output_path = f\"{model_path}outputs/\"\n",
    "os.makedirs(logits_output_path, exist_ok=True)\n",
    "os.makedirs(model_output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_logits(prot_acc_version, seq):\n",
    "    filepath = f\"{logits_output_path}{prot_acc_version}.pkl\"\n",
    "    if os.path.exists(filepath):\n",
    "        logits = pickle_utils.load_pickle(filepath) # numpy array of l x vocab_size=30\n",
    "    else: \n",
    "        with torch.no_grad():\n",
    "            logits = model.embed(seq) # l x vocab_size=30\n",
    "            pickle_utils.save_as_pickle(logits, filepath)\n",
    "    # print(logits.shape)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(data):\n",
    "    # data format: [(prot_id, seq)]\n",
    "    # print(data)\n",
    "    preds = []        \n",
    "    for i, (prot_acc_version, seq) in enumerate(data):\n",
    "        output_logits = compute_model_logits(prot_acc_version, seq) # l x vocab_size=30\n",
    "        indices = variants_df[variants_df[\"prot_acc_version\"]==prot_acc_version].index \n",
    "        # print(prot_acc_version, len(indices)) # indices can be of different shape for different runs, b/c we sample the singletons when computing variants_df\n",
    "        for idx in indices:\n",
    "            tuple = variants_df.loc[idx]\n",
    "            \n",
    "            wt_tok_idx = tokenizer.convert_tokens_to_ids(model.aa_prefix+tuple.wt)\n",
    "            mt_tok_idx = tokenizer.convert_tokens_to_ids(model.aa_prefix+tuple.mut)\n",
    "            pos = tuple.pos-1 #ncbi prot variants are 1 indexed, so <cls> is not at 0-position, so have to minus 1\n",
    "            \n",
    "            wt_logit = output_logits[pos][wt_tok_idx]\n",
    "            mt_logit = output_logits[pos][mt_tok_idx]\n",
    "            var_effect_score = mt_logit - wt_logit\n",
    "            tuple = dict(tuple)\n",
    "            tuple[\"pred\"] = var_effect_score\n",
    "            preds.append(tuple)\n",
    "            # print(preds)\n",
    "            # break\n",
    "            \n",
    "    preds_df = pd.DataFrame(preds)   \n",
    "    # print(preds_df)\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-of chunks: 3, 1st chunk size: 1\n",
      "Finished 0/3th chunk: (8, 9)\n",
      "Finished 1/3th chunk: (5, 9)\n",
      "Finished 2/3th chunk: (4, 9)\n",
      "Saving predictions ...\n",
      "(17, 9)\n",
      "  prot_acc_version  pos wt mut  wt_population  mut_poulation   wt_freq  \\\n",
      "0      NP_112509.3  236  D   N           4470              6  0.998660   \n",
      "1      NP_112509.3  195  R   H          55576            141  0.997469   \n",
      "2      NP_112509.3  374  H   R         202922           1874  0.990849   \n",
      "3      NP_112509.3  254  P   L          66634            206  0.996918   \n",
      "4      NP_112509.3  259  P   S          10680              1  0.999906   \n",
      "\n",
      "    mt_freq      pred  \n",
      "0  0.001340 -4.396527  \n",
      "1  0.002531 -5.587605  \n",
      "2  0.009151  0.419850  \n",
      "3  0.003082 -4.399530  \n",
      "4  0.000094 -4.352335  \n",
      "Time taken: 14.284538745880127 seconds\n"
     ]
    }
   ],
   "source": [
    "# if __name__==\"__main__\": # main worker\n",
    "start = time.time()\n",
    "is_cuda = torch.cuda.is_available()\n",
    "pred_dfs = []\n",
    "\n",
    "data = [(seq_record.id, str(seq_record.seq)) for seq_record in fasta_iterator]\n",
    "\n",
    "chunk_size = 32 if is_cuda else 1\n",
    "data_chunks = [data[x:x+chunk_size] for x in range(0, len(data), chunk_size)]\n",
    "data_chunks = data_chunks[:3] \n",
    "print(f\"#-of chunks: {len(data_chunks)}, 1st chunk size: {len(data_chunks[0])}\")\n",
    "\n",
    "\n",
    "# sequential run and debugging\n",
    "for i, data_chunk in enumerate(data_chunks):\n",
    "    pred_df = execute(data_chunk)\n",
    "    print(f\"Finished {i}/{len(data_chunks)}th chunk: {pred_df.shape}\")\n",
    "    pred_dfs.append(pred_df)\n",
    "\n",
    " # mpi run    \n",
    "# from mpi4py.futures import MPIPoolExecutor\n",
    "# executor = MPIPoolExecutor()\n",
    "# for i, pred_df in enumerate(executor.map(execute, data_chunks, unordered=True)):\n",
    "#     print(f\"Finished {i}/{len(data_chunks)}th chunk: {pred_df.shape}\")\n",
    "#     pred_dfs.append(pred_df)\n",
    "# executor.shutdown()\n",
    "\n",
    "\n",
    "result_df = pd.concat(pred_dfs)  \n",
    "print(\"Saving predictions ...\")  \n",
    "result_df.to_csv(f\"{model_output_path}popu_freq_preds_{model.name}.csv\", sep=\"\\t\", index=False, header=True)\n",
    "print(result_df.shape)\n",
    "print(result_df.head())\n",
    "\n",
    "print(f\"Time taken: {time.time()-start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopper_bioembeddings_dallago_from_source",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
