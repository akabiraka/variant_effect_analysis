{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "home_dir = \"../../\"\n",
    "module_path = os.path.abspath(os.path.join(home_dir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "# from models.bioembeddings_dallago.lm_heads.prottrans_lms_factory import load_prottrans_lm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16894, 33)\n",
      "Index(['mut_id', 'pmd_id', 'nr', 'crossref', 'uniprot_id', 'ensembl_id',\n",
      "       'taxid', 'protein', 'mut_PMD', 'mut_real', 'wt', 'mut', 'prot_pos',\n",
      "       'function_summarized', 'functional_effect', 'function', 'seq', 'snp_id',\n",
      "       'mrna_acc', 'mrna_ver', 'mrna_pos', 'allele', 'protein_acc',\n",
      "       'protein_ver', 'verified', 'chrom', 'chrom_pos', 'variation',\n",
      "       'variant_type', 'ref_allele', 'alt_allele', 'pmd_nr_id', 'pred'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_distribution(result_df, n_samples=None):\n",
    "    df = result_df.copy(deep=True)\n",
    "    plt.cla()\n",
    "\n",
    "    effect = df[df[\"class\"]==\"Effect\"]\n",
    "    knock_out = df[df[\"class\"]==\"Knock-out\"]\n",
    "    no_effect = df[df[\"class\"]==\"No-effect\"]\n",
    "\n",
    "    if n_samples is not None:\n",
    "        effect = effect.sample(n=n_samples)\n",
    "        knock_out = knock_out.sample(n=n_samples)\n",
    "        no_effect = no_effect.sample(n=n_samples)\n",
    "\n",
    "    # print(effect.shape, knock_out.shape, no_effect.shape)\n",
    "    bins = 100 # 20, 100, 500\n",
    "    \n",
    "    sns.histplot(knock_out[\"pred\"], label=f\"Knock-out\", element=\"step\", alpha=.2)\n",
    "    sns.histplot(no_effect[\"pred\"], label=f\"No-effect\", element=\"step\", alpha=.2)\n",
    "    sns.histplot(effect[\"pred\"], label=f\"Effect\", element=\"step\", alpha=.2)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Prediction scores (scaled)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "result_df = pd.read_csv(home_dir+f\"models/bioembeddings_dallago/outputs/prottrans_t5_bfd/pmd/preds_prottrans_t5_bfd.tsv\", sep=\"\\t\")\n",
    "result_df = result_df[result_df[\"crossref\"].apply(lambda x: True if \"HUMAN\" in x else False)] # number of human entries: 16996\n",
    "model_pred_col = \"pred\"\n",
    "result_df[\"pred\"]=(result_df[model_pred_col]-result_df[model_pred_col].min())/(result_df[model_pred_col].max()-result_df[model_pred_col].min()) # scaling prediction scores between [0, 1]\n",
    "print(result_df.shape)\n",
    "print(result_df.columns)\n",
    "result_df.rename(columns={\"functional_effect\": \"class\"}, inplace=True)\n",
    "result_df[\"class\"].value_counts()\n",
    "plot_distribution(result_df, n_samples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 512])\n",
      "torch.Size([1, 8, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 121)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bio_embeddings.embed import BeplerEmbedder\n",
    "belper_embedder = BeplerEmbedder()\n",
    "embedding = belper_embedder.embed(\"SEQVENCE\")\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLUS_RNN(\n",
      "  (embed): Embedding(22, 21, padding_idx=21)\n",
      "  (rnn): LSTM(21, 512, num_layers=3, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=1024, out_features=100, bias=True)\n",
      "  (decoder): Linear(in_features=1024, out_features=21, bias=True)\n",
      ")\n",
      "<plus.config.ModelConfig object at 0x7f65b5da87f0>\n",
      "<models.bioembeddings_dallago.lm_heads.plus_rnn_lm.PLUSRNNTokenizer object at 0x7f668a1f1700>\n",
      "[16  7  6 20  7  3  5  7]\n",
      "9\n",
      "False\n",
      "(8, 21)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from bio_embeddings.embed import PLUSRNNEmbedder\n",
    "from models.bioembeddings_dallago.lm_heads.plus_rnn_lm import PLUSRNNLM\n",
    "\n",
    "plusrnn_lm = PLUSRNNLM()\n",
    "print(plusrnn_lm._model)#.lm())\n",
    "print(plusrnn_lm._model_cfg)\n",
    "print(plusrnn_lm._tokenizer)\n",
    "print(plusrnn_lm._tokenizer.encode(\"SEQVENCE\".encode().upper()))\n",
    "print(plusrnn_lm._tokenizer.convert_tokens_to_ids(\"H\")) # 1-indexed\n",
    "\n",
    "logits = plusrnn_lm.embed(\"SEQVENCE\")\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_file': '/home/akabir4/.cache/bio_embeddings/plus_rnn/model_file'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plusrnn_lm._options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPCProtModel(\n",
      "  (enc): PatchedConvEncoder(\n",
      "    (embedding): Embedding(30, 32)\n",
      "    (conv1): Conv1d(32, 64, kernel_size=(4,), stride=(1,))\n",
      "    (norm1): ChannelNorm()\n",
      "    (conv2): Conv1d(64, 64, kernel_size=(6,), stride=(1,))\n",
      "    (norm2): ChannelNorm()\n",
      "    (conv3): Conv1d(64, 512, kernel_size=(3,), stride=(1,))\n",
      "    (norm3): ChannelNorm()\n",
      "  )\n",
      "  (autoregressor): GRUAutoregressor(\n",
      "    (gru): GRU(512, 512, batch_first=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import CPCProtEmbedder\n",
    "cpc_embedder = CPCProtEmbedder()\n",
    "print(cpc_embedder._model)\n",
    "print(cpc_embedder.tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPCProtEmbedding(\n",
      "  (cpc): CPCProtModel(\n",
      "    (enc): PatchedConvEncoder(\n",
      "      (embedding): Embedding(30, 32)\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(4,), stride=(1,))\n",
      "      (norm1): ChannelNorm()\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(6,), stride=(1,))\n",
      "      (norm2): ChannelNorm()\n",
      "      (conv3): Conv1d(64, 512, kernel_size=(3,), stride=(1,))\n",
      "      (norm3): ChannelNorm()\n",
      "    )\n",
      "    (autoregressor): GRUAutoregressor(\n",
      "      (gru): GRU(512, 512, batch_first=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "OrderedDict([('<pad>', 0), ('<mask>', 1), ('<cls>', 2), ('<sep>', 3), ('<unk>', 4), ('A', 5), ('B', 6), ('C', 7), ('D', 8), ('E', 9), ('F', 10), ('G', 11), ('H', 12), ('I', 13), ('K', 14), ('L', 15), ('M', 16), ('N', 17), ('O', 18), ('P', 19), ('Q', 20), ('R', 21), ('S', 22), ('T', 23), ('U', 24), ('V', 25), ('W', 26), ('X', 27), ('Y', 28), ('Z', 29)])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(cpc_embedder\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mvocab)\n\u001b[1;32m      3\u001b[0m embedding \u001b[39m=\u001b[39m cpc_embedder\u001b[39m.\u001b[39membed(\u001b[39m\"\u001b[39m\u001b[39mSEQVENCE\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(embedding\u001b[39m.\u001b[39;49mshape) \u001b[39m# (1, 512)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "embedding = cpc_embedder.embed(\"SEQVENCE\")\n",
    "print(embedding.shape) # (1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import SeqVecEmbedder # it took 13 mins to load 1st time\n",
    "embedder = SeqVecEmbedder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ElmoBiLm(\n",
      "  (_token_embedder): _ElmoCharacterEncoder(\n",
      "    (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
      "    (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
      "    (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
      "    (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
      "    (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
      "    (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
      "    (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
      "    (_highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (_elmo_lstm): ElmoLstm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "/home/akabir4/.cache/bio_embeddings/seqvec/options_file\n",
      "(3, 8, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(embedder._model.elmo_bilm)\n",
    "print(embedder._options_file) # from vim /home/akabir4/.cache/bio_embeddings/seqvec/options_file \"n_tokens_vocab\": 28\n",
    "embedding = embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # (3, 8, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m \u001b[39mimport\u001b[39;00m ProtTransBertBFDEmbedder\n\u001b[1;32m      2\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m e \u001b[39m=\u001b[39m ProtTransBertBFDEmbedder()\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/bio_embeddings/__init__.py:12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mThe functionality of bio_embeddings is split into 5 different modules\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m   bio_embeddings.visualize\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mextract\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproject\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/bio_embeddings/embed/__init__.py:68\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict, Type\n\u001b[1;32m     67\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membedder_interfaces\u001b[39;00m \u001b[39mimport\u001b[39;00m EmbedderInterface\n\u001b[0;32m---> 68\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfasttext_embedder\u001b[39;00m \u001b[39mimport\u001b[39;00m FastTextEmbedder\n\u001b[1;32m     69\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mglove_embedder\u001b[39;00m \u001b[39mimport\u001b[39;00m GloveEmbedder\n\u001b[1;32m     70\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mone_hot_encoding_embedder\u001b[39;00m \u001b[39mimport\u001b[39;00m OneHotEncodingEmbedder\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/bio_embeddings/embed/fasttext_embedder.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeyedvectors\u001b[39;00m \u001b[39mimport\u001b[39;00m KeyedVectors\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m ndarray\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membedder_interfaces\u001b[39;00m \u001b[39mimport\u001b[39;00m EmbedderInterface\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m4.3.0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m'\u001b[39m\u001b[39mgensim\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m logger\u001b[39m.\u001b[39mhandlers:  \u001b[39m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/gensim/parsing/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mporter\u001b[39;00m \u001b[39mimport\u001b[39;00m PorterStemmer  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     preprocess_documents,\n\u001b[1;32m      6\u001b[0m     preprocess_string,\n\u001b[1;32m      7\u001b[0m     read_file,\n\u001b[1;32m      8\u001b[0m     read_files,\n\u001b[1;32m      9\u001b[0m     remove_stopwords,\n\u001b[1;32m     10\u001b[0m     split_alphanum,\n\u001b[1;32m     11\u001b[0m     stem_text,\n\u001b[1;32m     12\u001b[0m     strip_multiple_whitespaces,\n\u001b[1;32m     13\u001b[0m     strip_non_alphanum,\n\u001b[1;32m     14\u001b[0m     strip_numeric,\n\u001b[1;32m     15\u001b[0m     strip_punctuation,\n\u001b[1;32m     16\u001b[0m     strip_short,\n\u001b[1;32m     17\u001b[0m     strip_tags,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/gensim/parsing/preprocessing.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mglob\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparsing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mporter\u001b[39;00m \u001b[39mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m     30\u001b[0m STOPWORDS \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m([\n\u001b[1;32m     31\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msix\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjust\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mless\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbeing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mindeed\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mover\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmove\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39manyway\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfour\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mown\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mthrough\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[39m'\u001b[39m\u001b[39musing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfifty\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwhere\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmill\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39monly\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfind\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbefore\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mone\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwhose\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msomewhere\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmake\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39monce\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     59\u001b[0m ])\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/gensim/utils.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_open\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mopen\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__ \u001b[39mas\u001b[39;00m gensim_version\n\u001b[1;32m     41\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/__init__.py:34\u001b[0m\n\u001b[1;32m     31\u001b[0m logger\u001b[39m.\u001b[39maddHandler(logging\u001b[39m.\u001b[39mNullHandler())\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_open\u001b[39;00m \u001b[39mimport\u001b[39;00m version  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msmart_open_lib\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mopen\u001b[39m, parse_uri, smart_open, register_compressor  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     36\u001b[0m _WARNING \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39msmart_open.s3_iter_bucket is deprecated and will stop functioning\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[39min a future version. Please import iter_bucket from the smart_open.s3 module instead:\u001b[39m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[39m    from smart_open.s3 import iter_bucket as s3_iter_bucket\u001b[39m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     42\u001b[0m _WARNED \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/smart_open_lib.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msmart_open\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlocal_file\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mso_file\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msmart_open\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mso_compression\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_open\u001b[39;00m \u001b[39mimport\u001b[39;00m doctools\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_open\u001b[39;00m \u001b[39mimport\u001b[39;00m transport\n\u001b[1;32m     38\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# For backwards compatibility and keeping old unit tests happy.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m#\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/doctools.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compression\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m transport\n\u001b[1;32m     23\u001b[0m PLACEHOLDER \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m    smart_open/doctools.py magic goes here\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_kwargs\u001b[39m(docstring):\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/transport.py:103\u001b[0m\n\u001b[1;32m    101\u001b[0m register_transport(\u001b[39m\"\u001b[39m\u001b[39msmart_open.gcs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m register_transport(\u001b[39m\"\u001b[39m\u001b[39msmart_open.hdfs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m register_transport(\u001b[39m\"\u001b[39;49m\u001b[39msmart_open.http\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    104\u001b[0m register_transport(\u001b[39m\"\u001b[39m\u001b[39msmart_open.s3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m register_transport(\u001b[39m\"\u001b[39m\u001b[39msmart_open.ssh\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/transport.py:49\u001b[0m, in \u001b[0;36mregister_transport\u001b[0;34m(submodule)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(submodule, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     48\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m         submodule \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(submodule)\n\u001b[1;32m     50\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-centos8-x86_64/gcc-9.3.0/python-3.8.6-ff/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/http.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     MISSING_DEPS \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/requests/__init__.py:43\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mRequests HTTP Library\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m~~~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m:license: Apache 2.0, see LICENSE for more details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib3\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[1;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/urllib3/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m exceptions\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_version\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconnectionpool\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPConnectionPool, HTTPSConnectionPool, connection_from_url\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfilepost\u001b[39;00m \u001b[39mimport\u001b[39;00m encode_multipart_formdata\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpoolmanager\u001b[39;00m \u001b[39mimport\u001b[39;00m PoolManager, ProxyManager, proxy_from_url\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/urllib3/connectionpool.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msocket\u001b[39;00m \u001b[39mimport\u001b[39;00m error \u001b[39mas\u001b[39;00m SocketError\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msocket\u001b[39;00m \u001b[39mimport\u001b[39;00m timeout \u001b[39mas\u001b[39;00m SocketTimeout\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     BaseSSLError,\n\u001b[1;32m     14\u001b[0m     \u001b[39mBrokenPipeError\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     DummyConnection,\n\u001b[1;32m     16\u001b[0m     HTTPConnection,\n\u001b[1;32m     17\u001b[0m     HTTPException,\n\u001b[1;32m     18\u001b[0m     HTTPSConnection,\n\u001b[1;32m     19\u001b[0m     VerifiedHTTPSConnection,\n\u001b[1;32m     20\u001b[0m     port_by_scheme,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     ClosedPoolError,\n\u001b[1;32m     24\u001b[0m     EmptyPoolError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39mTimeoutError\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpackages\u001b[39;00m \u001b[39mimport\u001b[39;00m six\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/urllib3/connection.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpackages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msix\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmoves\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhttp_client\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPConnection \u001b[39mas\u001b[39;00m _HTTPConnection\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpackages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msix\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmoves\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhttp_client\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPException  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproxy\u001b[39;00m \u001b[39mimport\u001b[39;00m create_proxy_ssl_context\n\u001b[1;32m     17\u001b[0m \u001b[39mtry\u001b[39;00m:  \u001b[39m# Compiled with SSL?\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mssl\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/urllib3/util/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m \u001b[39mimport\u001b[39;00m SKIP_HEADER, SKIPPABLE_HEADERS, make_headers\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m is_fp_closed\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mretry\u001b[39;00m \u001b[39mimport\u001b[39;00m Retry\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mssl_\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     ALPN_PROTOCOLS,\n\u001b[1;32m     10\u001b[0m     HAS_SNI,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     ssl_wrap_socket,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtimeout\u001b[39;00m \u001b[39mimport\u001b[39;00m Timeout, current_time\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:779\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:874\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:972\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
    "start = time.time()\n",
    "e = ProtTransBertBFDEmbedder()\n",
    "print(e._model_directory)\n",
    "# print(e._model)\n",
    "# print(e._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(e._tokenizer.convert_tokens_to_ids(\"A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 30)\n"
     ]
    }
   ],
   "source": [
    "embedding = e.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # 8, 1024; 0-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alhumdulillah\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/akabir4/.cache/bio_embeddings/prottrans_albert_bfd/model_directory were not used when initializing AlbertForMaskedLM: ['sop_classifier.classifier.bias', 'sop_classifier.classifier.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akabir4/.cache/bio_embeddings/prottrans_albert_bfd/model_directory\n",
      "AlbertForMaskedLM(\n",
      "  (albert): AlbertModel(\n",
      "    (embeddings): AlbertEmbeddings(\n",
      "      (word_embeddings): Embedding(34, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(40000, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): AlbertTransformer(\n",
      "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=4096, bias=True)\n",
      "      (albert_layer_groups): ModuleList(\n",
      "        (0): AlbertLayerGroup(\n",
      "          (albert_layers): ModuleList(\n",
      "            (0): AlbertLayer(\n",
      "              (full_layer_layer_norm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): AlbertAttention(\n",
      "                (query): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "                (key): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "                (value): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "                (attention_dropout): Dropout(p=0, inplace=False)\n",
      "                (output_dropout): Dropout(p=0, inplace=False)\n",
      "                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "                (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "              (ffn): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "              (ffn_output): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (predictions): AlbertMLMHead(\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    (decoder): Linear(in_features=128, out_features=34, bias=True)\n",
      "    (activation): GELUActivation()\n",
      "  )\n",
      ")\n",
      "AlbertTokenizer(name_or_path='', vocab_size=34, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '<unk>', 'sep_token': '[SEP]', 'pad_token': '<pad>', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n",
      "Time taken to load model in CPU: 2.240201711654663 s\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransAlbertBFDEmbedder\n",
    "start = time.time()\n",
    "albert_embedder = ProtTransAlbertBFDEmbedder()\n",
    "print(albert_embedder._model_directory)\n",
    "print(albert_embedder._model)\n",
    "print(albert_embedder._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 34])\n",
      "(8, 34)\n"
     ]
    }
   ],
   "source": [
    "embedding = albert_embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # 8, 1024; 0-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alhumdulillah\n",
      "XLNetLMHeadModel(\n",
      "  (transformer): XLNetModel(\n",
      "    (word_embedding): Embedding(37, 1024)\n",
      "    (layer): ModuleList(\n",
      "      (0): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (6): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (7): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (8): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (9): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (10): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (11): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (12): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (13): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (14): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (15): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (16): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (17): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (18): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (19): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (20): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (21): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (22): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (23): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (24): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (25): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (26): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (27): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (28): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (29): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_loss): Linear(in_features=1024, out_features=37, bias=True)\n",
      ")\n",
      "XLNetTokenizer(name_or_path='/home/akabir4/.cache/bio_embeddings/prottrans_xlnet_uniref100/model_directory/spm_model.model', vocab_size=37, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<eop>', '<eod>']})\n",
      "Time taken to load model in CPU: 4.296059846878052 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1706: FutureWarning: Calling XLNetTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransXLNetUniRef100Embedder\n",
    "start = time.time()\n",
    "xlnet_embedder = ProtTransXLNetUniRef100Embedder()\n",
    "print(xlnet_embedder._model)\n",
    "print(xlnet_embedder._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 37)\n"
     ]
    }
   ],
   "source": [
    "embedding = xlnet_embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # 8, 1024; 0-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alhumdulillah\n",
      "True\n",
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=128, bias=False)\n",
      ")\n",
      "T5Tokenizer(name_or_path='/home/akabir4/.cache/bio_embeddings/prottrans_t5_bfd/model_directory', vocab_size=128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})\n",
      "Time taken to load model in CPU: 32.91669464111328 s\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransT5BFDEmbedder\n",
    "start = time.time()\n",
    "t5bdf_embedder = ProtTransT5BFDEmbedder()\n",
    "print(t5bdf_embedder._model)\n",
    "print(t5bdf_embedder._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 128])\n",
      "(8, 128)\n"
     ]
    }
   ],
   "source": [
    "embedding = t5bdf_embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alhumdulillah\n",
      "True\n",
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=128, bias=False)\n",
      ")\n",
      "T5Tokenizer(name_or_path='/home/akabir4/.cache/bio_embeddings/prottrans_t5_uniref50/model_directory', vocab_size=128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})\n",
      "Time taken to load model in CPU: 44.4154257774353 s\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransT5UniRef50Embedder\n",
    "start = time.time()\n",
    "t5uniref_embedder = ProtTransT5UniRef50Embedder()\n",
    "print(t5uniref_embedder._model)\n",
    "print(t5uniref_embedder._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 128])\n",
      "(8, 128)\n"
     ]
    }
   ],
   "source": [
    "embedding = t5uniref_embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alhumdulillah\n",
      "True\n",
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=128, bias=False)\n",
      ")\n",
      "T5Tokenizer(name_or_path='/home/akabir4/.cache/bio_embeddings/prottrans_t5_uniref50/model_directory', vocab_size=128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})\n",
      "Time taken to load model in CPU: 31.742605209350586 s\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransT5XLU50Embedder\n",
    "start = time.time()\n",
    "t5xl_embedder = ProtTransT5UniRef50Embedder()\n",
    "print(t5xl_embedder._model)\n",
    "print(t5xl_embedder._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 128])\n",
      "(8, 128)\n"
     ]
    }
   ],
   "source": [
    "embedding = t5xl_embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopper_bioembeddings_dallago",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
