{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "home_dir = \"../../\"\n",
    "module_path = os.path.abspath(os.path.join(home_dir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from models.bioembeddings_dallago.lm_heads.prottrans_lms_factory import load_prottrans_lm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 512])\n",
      "torch.Size([1, 8, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 121)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bio_embeddings.embed import BeplerEmbedder\n",
    "belper_embedder = BeplerEmbedder()\n",
    "embedding = belper_embedder.embed(\"SEQVENCE\")\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLUS_RNN(\n",
      "  (embed): Embedding(22, 21, padding_idx=21)\n",
      "  (rnn): LSTM(21, 512, num_layers=3, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=1024, out_features=100, bias=True)\n",
      "  (decoder): Linear(in_features=1024, out_features=21, bias=True)\n",
      ")\n",
      "<plus.config.ModelConfig object at 0x7f65b5da87f0>\n",
      "<models.bioembeddings_dallago.lm_heads.plus_rnn_lm.PLUSRNNTokenizer object at 0x7f668a1f1700>\n",
      "[16  7  6 20  7  3  5  7]\n",
      "9\n",
      "False\n",
      "(8, 21)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from bio_embeddings.embed import PLUSRNNEmbedder\n",
    "from models.bioembeddings_dallago.lm_heads.plus_rnn_lm import PLUSRNNLM\n",
    "\n",
    "plusrnn_lm = PLUSRNNLM()\n",
    "print(plusrnn_lm._model)#.lm())\n",
    "print(plusrnn_lm._model_cfg)\n",
    "print(plusrnn_lm._tokenizer)\n",
    "print(plusrnn_lm._tokenizer.encode(\"SEQVENCE\".encode().upper()))\n",
    "print(plusrnn_lm._tokenizer.convert_tokens_to_ids(\"H\")) # 1-indexed\n",
    "\n",
    "logits = plusrnn_lm.embed(\"SEQVENCE\")\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_file': '/home/akabir4/.cache/bio_embeddings/plus_rnn/model_file'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plusrnn_lm._options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPCProtModel(\n",
      "  (enc): PatchedConvEncoder(\n",
      "    (embedding): Embedding(30, 32)\n",
      "    (conv1): Conv1d(32, 64, kernel_size=(4,), stride=(1,))\n",
      "    (norm1): ChannelNorm()\n",
      "    (conv2): Conv1d(64, 64, kernel_size=(6,), stride=(1,))\n",
      "    (norm2): ChannelNorm()\n",
      "    (conv3): Conv1d(64, 512, kernel_size=(3,), stride=(1,))\n",
      "    (norm3): ChannelNorm()\n",
      "  )\n",
      "  (autoregressor): GRUAutoregressor(\n",
      "    (gru): GRU(512, 512, batch_first=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import CPCProtEmbedder\n",
    "cpc_embedder = CPCProtEmbedder()\n",
    "print(cpc_embedder._model)\n",
    "print(cpc_embedder.tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPCProtEmbedding(\n",
      "  (cpc): CPCProtModel(\n",
      "    (enc): PatchedConvEncoder(\n",
      "      (embedding): Embedding(30, 32)\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(4,), stride=(1,))\n",
      "      (norm1): ChannelNorm()\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(6,), stride=(1,))\n",
      "      (norm2): ChannelNorm()\n",
      "      (conv3): Conv1d(64, 512, kernel_size=(3,), stride=(1,))\n",
      "      (norm3): ChannelNorm()\n",
      "    )\n",
      "    (autoregressor): GRUAutoregressor(\n",
      "      (gru): GRU(512, 512, batch_first=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "OrderedDict([('<pad>', 0), ('<mask>', 1), ('<cls>', 2), ('<sep>', 3), ('<unk>', 4), ('A', 5), ('B', 6), ('C', 7), ('D', 8), ('E', 9), ('F', 10), ('G', 11), ('H', 12), ('I', 13), ('K', 14), ('L', 15), ('M', 16), ('N', 17), ('O', 18), ('P', 19), ('Q', 20), ('R', 21), ('S', 22), ('T', 23), ('U', 24), ('V', 25), ('W', 26), ('X', 27), ('Y', 28), ('Z', 29)])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(cpc_embedder\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mvocab)\n\u001b[1;32m      3\u001b[0m embedding \u001b[39m=\u001b[39m cpc_embedder\u001b[39m.\u001b[39membed(\u001b[39m\"\u001b[39m\u001b[39mSEQVENCE\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(embedding\u001b[39m.\u001b[39;49mshape) \u001b[39m# (1, 512)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "embedding = cpc_embedder.embed(\"SEQVENCE\")\n",
    "print(embedding.shape) # (1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import SeqVecEmbedder # it took 13 mins to load 1st time\n",
    "embedder = SeqVecEmbedder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ElmoBiLm(\n",
      "  (_token_embedder): _ElmoCharacterEncoder(\n",
      "    (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
      "    (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
      "    (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
      "    (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
      "    (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
      "    (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
      "    (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
      "    (_highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (_elmo_lstm): ElmoLstm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "/home/akabir4/.cache/bio_embeddings/seqvec/options_file\n",
      "(3, 8, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(embedder._model.elmo_bilm)\n",
    "print(embedder._options_file) # from vim /home/akabir4/.cache/bio_embeddings/seqvec/options_file \"n_tokens_vocab\": 28\n",
    "embedding = embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # (3, 8, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m \u001b[39mimport\u001b[39;00m ProtTransBertBFDEmbedder\n\u001b[1;32m      2\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m e \u001b[39m=\u001b[39m ProtTransBertBFDEmbedder()\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/bio_embeddings/__init__.py:12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mThe functionality of bio_embeddings is split into 5 different modules\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m   bio_embeddings.visualize\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mextract\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproject\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/bio_embeddings/embed/__init__.py:68\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict, Type\n\u001b[1;32m     67\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membedder_interfaces\u001b[39;00m \u001b[39mimport\u001b[39;00m EmbedderInterface\n\u001b[0;32m---> 68\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfasttext_embedder\u001b[39;00m \u001b[39mimport\u001b[39;00m FastTextEmbedder\n\u001b[1;32m     69\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mglove_embedder\u001b[39;00m \u001b[39mimport\u001b[39;00m GloveEmbedder\n\u001b[1;32m     70\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mone_hot_encoding_embedder\u001b[39;00m \u001b[39mimport\u001b[39;00m OneHotEncodingEmbedder\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/bio_embeddings/embed/fasttext_embedder.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeyedvectors\u001b[39;00m \u001b[39mimport\u001b[39;00m KeyedVectors\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m ndarray\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbio_embeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membedder_interfaces\u001b[39;00m \u001b[39mimport\u001b[39;00m EmbedderInterface\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m4.3.0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m'\u001b[39m\u001b[39mgensim\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m logger\u001b[39m.\u001b[39mhandlers:  \u001b[39m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/gensim/parsing/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mporter\u001b[39;00m \u001b[39mimport\u001b[39;00m PorterStemmer  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     preprocess_documents,\n\u001b[1;32m      6\u001b[0m     preprocess_string,\n\u001b[1;32m      7\u001b[0m     read_file,\n\u001b[1;32m      8\u001b[0m     read_files,\n\u001b[1;32m      9\u001b[0m     remove_stopwords,\n\u001b[1;32m     10\u001b[0m     split_alphanum,\n\u001b[1;32m     11\u001b[0m     stem_text,\n\u001b[1;32m     12\u001b[0m     strip_multiple_whitespaces,\n\u001b[1;32m     13\u001b[0m     strip_non_alphanum,\n\u001b[1;32m     14\u001b[0m     strip_numeric,\n\u001b[1;32m     15\u001b[0m     strip_punctuation,\n\u001b[1;32m     16\u001b[0m     strip_short,\n\u001b[1;32m     17\u001b[0m     strip_tags,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/gensim/parsing/preprocessing.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mglob\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparsing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mporter\u001b[39;00m \u001b[39mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m     30\u001b[0m STOPWORDS \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m([\n\u001b[1;32m     31\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msix\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjust\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mless\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbeing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mindeed\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mover\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmove\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39manyway\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfour\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnot\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mown\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mthrough\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[39m'\u001b[39m\u001b[39musing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfifty\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwhere\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmill\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39monly\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfind\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbefore\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mone\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwhose\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msomewhere\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmake\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39monce\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     59\u001b[0m ])\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/gensim/utils.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_open\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mopen\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__ \u001b[39mas\u001b[39;00m gensim_version\n\u001b[1;32m     41\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/__init__.py:34\u001b[0m\n\u001b[1;32m     31\u001b[0m logger\u001b[39m.\u001b[39maddHandler(logging\u001b[39m.\u001b[39mNullHandler())\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_open\u001b[39;00m \u001b[39mimport\u001b[39;00m version  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msmart_open_lib\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mopen\u001b[39m, parse_uri, smart_open, register_compressor  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     36\u001b[0m _WARNING \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39msmart_open.s3_iter_bucket is deprecated and will stop functioning\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[39min a future version. Please import iter_bucket from the smart_open.s3 module instead:\u001b[39m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[39m    from smart_open.s3 import iter_bucket as s3_iter_bucket\u001b[39m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     42\u001b[0m _WARNED \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/smart_open_lib.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msmart_open\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlocal_file\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mso_file\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msmart_open\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mso_compression\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_open\u001b[39;00m \u001b[39mimport\u001b[39;00m doctools\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msmart_open\u001b[39;00m \u001b[39mimport\u001b[39;00m transport\n\u001b[1;32m     38\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# For backwards compatibility and keeping old unit tests happy.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m#\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/doctools.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compression\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m transport\n\u001b[1;32m     23\u001b[0m PLACEHOLDER \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m    smart_open/doctools.py magic goes here\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_kwargs\u001b[39m(docstring):\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/transport.py:103\u001b[0m\n\u001b[1;32m    101\u001b[0m register_transport(\u001b[39m\"\u001b[39m\u001b[39msmart_open.gcs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m register_transport(\u001b[39m\"\u001b[39m\u001b[39msmart_open.hdfs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m register_transport(\u001b[39m\"\u001b[39;49m\u001b[39msmart_open.http\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    104\u001b[0m register_transport(\u001b[39m\"\u001b[39m\u001b[39msmart_open.s3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m register_transport(\u001b[39m\"\u001b[39m\u001b[39msmart_open.ssh\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/transport.py:49\u001b[0m, in \u001b[0;36mregister_transport\u001b[0;34m(submodule)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(submodule, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     48\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m         submodule \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(submodule)\n\u001b[1;32m     50\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-centos8-x86_64/gcc-9.3.0/python-3.8.6-ff/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/smart_open/http.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     MISSING_DEPS \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/requests/__init__.py:43\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mRequests HTTP Library\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m~~~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m:license: Apache 2.0, see LICENSE for more details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib3\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[1;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/urllib3/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m exceptions\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_version\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconnectionpool\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPConnectionPool, HTTPSConnectionPool, connection_from_url\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfilepost\u001b[39;00m \u001b[39mimport\u001b[39;00m encode_multipart_formdata\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpoolmanager\u001b[39;00m \u001b[39mimport\u001b[39;00m PoolManager, ProxyManager, proxy_from_url\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/urllib3/connectionpool.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msocket\u001b[39;00m \u001b[39mimport\u001b[39;00m error \u001b[39mas\u001b[39;00m SocketError\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msocket\u001b[39;00m \u001b[39mimport\u001b[39;00m timeout \u001b[39mas\u001b[39;00m SocketTimeout\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     BaseSSLError,\n\u001b[1;32m     14\u001b[0m     \u001b[39mBrokenPipeError\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     DummyConnection,\n\u001b[1;32m     16\u001b[0m     HTTPConnection,\n\u001b[1;32m     17\u001b[0m     HTTPException,\n\u001b[1;32m     18\u001b[0m     HTTPSConnection,\n\u001b[1;32m     19\u001b[0m     VerifiedHTTPSConnection,\n\u001b[1;32m     20\u001b[0m     port_by_scheme,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     ClosedPoolError,\n\u001b[1;32m     24\u001b[0m     EmptyPoolError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39mTimeoutError\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpackages\u001b[39;00m \u001b[39mimport\u001b[39;00m six\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/urllib3/connection.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpackages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msix\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmoves\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhttp_client\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPConnection \u001b[39mas\u001b[39;00m _HTTPConnection\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpackages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msix\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmoves\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhttp_client\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPException  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproxy\u001b[39;00m \u001b[39mimport\u001b[39;00m create_proxy_ssl_context\n\u001b[1;32m     17\u001b[0m \u001b[39mtry\u001b[39;00m:  \u001b[39m# Compiled with SSL?\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mssl\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/urllib3/util/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m \u001b[39mimport\u001b[39;00m SKIP_HEADER, SKIPPABLE_HEADERS, make_headers\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m is_fp_closed\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mretry\u001b[39;00m \u001b[39mimport\u001b[39;00m Retry\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mssl_\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     ALPN_PROTOCOLS,\n\u001b[1;32m     10\u001b[0m     HAS_SNI,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     ssl_wrap_socket,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtimeout\u001b[39;00m \u001b[39mimport\u001b[39;00m Timeout, current_time\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:779\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:874\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:972\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
    "start = time.time()\n",
    "e = ProtTransBertBFDEmbedder()\n",
    "print(e._model_directory)\n",
    "# print(e._model)\n",
    "# print(e._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(e._tokenizer.convert_tokens_to_ids(\"A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 30)\n"
     ]
    }
   ],
   "source": [
    "embedding = e.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # 8, 1024; 0-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alhumdulillah\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/akabir4/.cache/bio_embeddings/prottrans_albert_bfd/model_directory were not used when initializing AlbertForMaskedLM: ['sop_classifier.classifier.bias', 'sop_classifier.classifier.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akabir4/.cache/bio_embeddings/prottrans_albert_bfd/model_directory\n",
      "AlbertForMaskedLM(\n",
      "  (albert): AlbertModel(\n",
      "    (embeddings): AlbertEmbeddings(\n",
      "      (word_embeddings): Embedding(34, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(40000, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): AlbertTransformer(\n",
      "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=4096, bias=True)\n",
      "      (albert_layer_groups): ModuleList(\n",
      "        (0): AlbertLayerGroup(\n",
      "          (albert_layers): ModuleList(\n",
      "            (0): AlbertLayer(\n",
      "              (full_layer_layer_norm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): AlbertAttention(\n",
      "                (query): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "                (key): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "                (value): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "                (attention_dropout): Dropout(p=0, inplace=False)\n",
      "                (output_dropout): Dropout(p=0, inplace=False)\n",
      "                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "                (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "              (ffn): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "              (ffn_output): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (predictions): AlbertMLMHead(\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "    (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
      "    (decoder): Linear(in_features=128, out_features=34, bias=True)\n",
      "    (activation): GELUActivation()\n",
      "  )\n",
      ")\n",
      "AlbertTokenizer(name_or_path='', vocab_size=34, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '<unk>', 'sep_token': '[SEP]', 'pad_token': '<pad>', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n",
      "Time taken to load model in CPU: 2.240201711654663 s\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransAlbertBFDEmbedder\n",
    "start = time.time()\n",
    "albert_embedder = ProtTransAlbertBFDEmbedder()\n",
    "print(albert_embedder._model_directory)\n",
    "print(albert_embedder._model)\n",
    "print(albert_embedder._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 34])\n",
      "(8, 34)\n"
     ]
    }
   ],
   "source": [
    "embedding = albert_embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # 8, 1024; 0-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alhumdulillah\n",
      "XLNetLMHeadModel(\n",
      "  (transformer): XLNetModel(\n",
      "    (word_embedding): Embedding(37, 1024)\n",
      "    (layer): ModuleList(\n",
      "      (0): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (6): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (7): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (8): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (9): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (10): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (11): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (12): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (13): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (14): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (15): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (16): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (17): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (18): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (19): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (20): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (21): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (22): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (23): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (24): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (25): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (26): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (27): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (28): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (29): XLNetLayer(\n",
      "        (rel_attn): XLNetRelativeAttention(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ff): XLNetFeedForward(\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation_function): ReLU()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_loss): Linear(in_features=1024, out_features=37, bias=True)\n",
      ")\n",
      "XLNetTokenizer(name_or_path='/home/akabir4/.cache/bio_embeddings/prottrans_xlnet_uniref100/model_directory/spm_model.model', vocab_size=37, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<eop>', '<eod>']})\n",
      "Time taken to load model in CPU: 4.296059846878052 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1706: FutureWarning: Calling XLNetTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransXLNetUniRef100Embedder\n",
    "start = time.time()\n",
    "xlnet_embedder = ProtTransXLNetUniRef100Embedder()\n",
    "print(xlnet_embedder._model)\n",
    "print(xlnet_embedder._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 37)\n"
     ]
    }
   ],
   "source": [
    "embedding = xlnet_embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # 8, 1024; 0-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_bioembeddings_dallago_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alhumdulillah\n",
      "True\n",
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=128, bias=False)\n",
      ")\n",
      "T5Tokenizer(name_or_path='/home/akabir4/.cache/bio_embeddings/prottrans_t5_bfd/model_directory', vocab_size=128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})\n",
      "Time taken to load model in CPU: 32.91669464111328 s\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransT5BFDEmbedder\n",
    "start = time.time()\n",
    "t5bdf_embedder = ProtTransT5BFDEmbedder()\n",
    "print(t5bdf_embedder._model)\n",
    "print(t5bdf_embedder._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 128])\n",
      "(8, 128)\n"
     ]
    }
   ],
   "source": [
    "embedding = t5bdf_embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alhumdulillah\n",
      "True\n",
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=128, bias=False)\n",
      ")\n",
      "T5Tokenizer(name_or_path='/home/akabir4/.cache/bio_embeddings/prottrans_t5_uniref50/model_directory', vocab_size=128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})\n",
      "Time taken to load model in CPU: 44.4154257774353 s\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransT5UniRef50Embedder\n",
    "start = time.time()\n",
    "t5uniref_embedder = ProtTransT5UniRef50Embedder()\n",
    "print(t5uniref_embedder._model)\n",
    "print(t5uniref_embedder._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 128])\n",
      "(8, 128)\n"
     ]
    }
   ],
   "source": [
    "embedding = t5uniref_embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alhumdulillah\n",
      "True\n",
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=128, bias=False)\n",
      ")\n",
      "T5Tokenizer(name_or_path='/home/akabir4/.cache/bio_embeddings/prottrans_t5_uniref50/model_directory', vocab_size=128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})\n",
      "Time taken to load model in CPU: 31.742605209350586 s\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed import ProtTransT5XLU50Embedder\n",
    "start = time.time()\n",
    "t5xl_embedder = ProtTransT5UniRef50Embedder()\n",
    "print(t5xl_embedder._model)\n",
    "print(t5xl_embedder._tokenizer)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to load model in CPU: {end-start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 128])\n",
      "(8, 128)\n"
     ]
    }
   ],
   "source": [
    "embedding = t5xl_embedder.embed(\"SEQVENCE\")\n",
    "print(np.array(embedding).shape) # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopper_bioembeddings_dallago",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
