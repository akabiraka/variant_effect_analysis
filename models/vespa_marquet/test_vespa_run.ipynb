{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_vespa_marquet_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "home_dir = \"../../\"\n",
    "module_path = os.path.abspath(os.path.join(home_dir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from vespa.predict.config import DEVICE, MODEL_PATH_DICT, EMBEDDING_HALF_PREC\n",
    "print(EMBEDDING_HALF_PREC)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: embedding generation\n",
    "from vespa.predict.embedding import T5_Embed\n",
    "t5_emb = T5_Embed(cache_dir=home_dir+\"models/vespa_marquet/cache\")\n",
    "# EMBEDDING_HALF_PREC must be set False for t5_emb model to run\n",
    "model, tokenizer = t5_emb.prott5.get_model(0) # EMBED=0\n",
    "\n",
    "def compute_embedding(seq):\n",
    "    seq_len = len(seq)\n",
    "    seq = ' '.join(list(seq))\n",
    "    token_encoding = tokenizer([seq], add_special_tokens=True, padding='longest', return_tensors=\"pt\")\n",
    "    input_ids = token_encoding['input_ids'].to(DEVICE)\n",
    "    attention_mask = token_encoding['attention_mask'].to(DEVICE)\n",
    "    # print(input_ids, attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding_repr = model(input_ids, attention_mask=attention_mask) #1 x seq_len x embedding_dim\n",
    "        emb = embedding_repr.last_hidden_state[0, :seq_len]\n",
    "        emb = emb.detach().cpu().numpy().squeeze() # seq_len, 1024\n",
    "        # print(emb.shape)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: conservation prediction\n",
    "from vespa.predict.conspred import ProtT5Cons\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = Path(MODEL_PATH_DICT[\"CONSCNN\"])\n",
    "conspred = ProtT5Cons(checkpoint_path)\n",
    "# print(checkpoint_path)\n",
    "# conspred.predictor\n",
    "\n",
    "def compute_conservation(embedding):\n",
    "    with torch.no_grad():\n",
    "        Yhat = conspred.predictor(torch.tensor(embedding).unsqueeze(0))\n",
    "        prob = conspred.model.extract_probabilities(Yhat)\n",
    "        # cls = conspred.model.extract_conservation_score(Yhat)\n",
    "\n",
    "    # Yhat = Yhat.squeeze(0).detach().cpu().numpy()\n",
    "    prob = prob.squeeze(0).detach().cpu().numpy()\n",
    "    # cls = cls.squeeze(0).detach().cpu().numpy()\n",
    "    # print(Yhat.shape, prob.shape, cls.shape) # shapes: (9, seq_len) (9, seq_len) (seq_len,)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: computing log-odds\n",
    "from vespa.predict.logodds import T5_condProbas\n",
    "t5_condProbas = T5_condProbas(cache_dir=home_dir+\"models/vespa_marquet/cache\")\n",
    "\n",
    "def get_log_odds(seq_dict, mutation_generator):\n",
    "    proba_dict = t5_condProbas.get_proba_dict(seq_dict, mutation_generator)\n",
    "    dmiss_data = t5_condProbas.get_log_odds(proba_dict) # seq_len, 20. DMISS (Deep mutational in-silico scanning.)\n",
    "    return dmiss_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_vespa_marquet_from_source/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.22.2.post1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from vespa.predict.vespa import VespaPred\n",
    "is_vespa=True\n",
    "vespa_predictor = VespaPred(vespa=is_vespa, vespal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19,  9, 17, 15, 16, 18,  7,  6, 16,  4,  7, 10, 16, 23, 21,  3,  1]]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract Sequence Logodds: 1it [00:00,  1.11it/s]\n",
      "Logodds Lookup: 100%|██████████| 2/2 [00:00<00:00, 28728.11it/s]\n",
      "Blosum Lookup: 100%|██████████| 2/2 [00:00<00:00, 20213.51it/s]\n",
      "Conservation Lookup: 100%|██████████| 2/2 [00:00<00:00, 39568.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Model Predictions\n",
      "Predictions Done; Generate output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info Generation: 100%|██████████| 2/2 [00:00<00:00, 49636.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prot': [('M0C', {'VESPAl': 0.5799001754919151, 'VESPA': 0.5011233500844459}),\n",
       "  ('E1S', {'VESPAl': 0.4177296331552022, 'VESPA': 0.32330920524589424})]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vespa.predict.utils import MutationGenerator\n",
    "\n",
    "\n",
    "protid = \"prot\"\n",
    "seq_dict = {protid: \"MENFQYSVQLSDQXWA\"}\n",
    "mutations_dict = {protid: [\"M1C\", \"E2S\"]}\n",
    "seq = seq_dict[protid]\n",
    "\n",
    "temp_mutations_filepath = home_dir+\"models/vespa_marquet/cache/temp_mutations.txt\"\n",
    "temp_protid = \"protid\"\n",
    "with open(temp_mutations_filepath, \"w\") as f:\n",
    "    for mutation in mutations_dict[protid]:\n",
    "        f.write(f\"{temp_protid}_{mutation}\\n\")\n",
    "temp_seq_dict = {temp_protid: seq}\n",
    "\n",
    "mutations_file_path = Path(temp_mutations_filepath)\n",
    "mutation_generator = MutationGenerator(temp_seq_dict, file_path=mutations_file_path, one_based_file=True)\n",
    "\n",
    "\n",
    "embedding = compute_embedding(seq) # shape: seq_len, 1024\n",
    "conservation = compute_conservation(embedding)\n",
    "conservation_dict = {temp_protid: conservation}\n",
    "\n",
    "if is_vespa:\n",
    "    log_odds = get_log_odds(temp_seq_dict, mutation_generator)\n",
    "    predictions = vespa_predictor.generate_predictions(mutation_generator, conservation_dict, log_odds)\n",
    "else: \n",
    "    predictions = vespa_predictor.generate_predictions(mutation_generator, conservation_dict)\n",
    "\n",
    "predictions[protid] = predictions.pop(temp_protid)\n",
    "predictions # this result is exactly same in the vespa_outs_from_cmd/0.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopper_vespa_marquet_from_source",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
