{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_vespa_marquet_from_source/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "home_dir = \"../../\"\n",
    "module_path = os.path.abspath(os.path.join(home_dir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from vespa.predict.logodds import *\n",
    "import vespa.predict.utils as vespa_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir=home_dir+\"models/vespa_marquet/cache\"\n",
    "t5_condProbas = T5_condProbas(cache_dir=cache_dir)\n",
    "model, tokenizer = t5_condProbas.prott5.get_model(1) #1=LOGODDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3, 19,  6, 17,  7, 11, 20,  8,  1]])\n",
      "(9, 128)\n"
     ]
    }
   ],
   "source": [
    "seq = \"AMVNSTHR\" # eos token added\n",
    "seq = \" \".join(list(seq))\n",
    "model.to(\"cpu\")\n",
    "input_ids = tokenizer(seq, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "print(input_ids)\n",
    "with torch.no_grad():  # only logits is useful for us\n",
    "    logits = model(input_ids, labels=input_ids).logits\n",
    "    logits = logits[0].detach().numpy() \n",
    "\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"â–V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MVNSTHRGMHTSLHLWNRSSYRLHSNASESLGKGYSDGGCYEQLFVSPEVFVTLGVISLLENILVIVAIAKNKNLHSPMYFFICSLAVADMLVSVSNGSETIVITLLNSTDTDAQSFTVNIDNVIDSVICSSLLASICSLLSIAVDRYFTIFYALQYHNIMTVKRVGIIISCIWAACTVSGILFIIYSDSSAVIICLITMFFTMLALMASLYVHMFLMARLHIKRIAVLPGTGAIRQGANMKGAITLTILIGVFVVCWAPFFLHLIFYISCPQNPYCVCFMSHFNLYLILIMCNSIIDPLIYALRSQELRKTFKEIICCYPLGGLCDLSSRY'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "seq_dict = vespa_utils.parse_fasta_input(Path(home_dir+\"models/aa_common/datasets_pmd_analysis/pmd_sequences.fasta\")) # this replaces the unknown amino acids UZO, not B\n",
    "print(len(seq_dict))\n",
    "protid_seq_dict_list = [(prot_id, seq) for prot_id, seq in  seq_dict.items()]\n",
    "protid, seq = protid_seq_dict_list[0]\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 14.76 GiB total capacity; 9.02 GiB already allocated; 17.75 MiB free; 9.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m proba_dict \u001b[39m=\u001b[39m t5_condProbas\u001b[39m.\u001b[39;49mget_proba_dict(protid_seq_dict_list[\u001b[39m0\u001b[39;49m], mutation_gen)\n\u001b[1;32m      2\u001b[0m \u001b[39m# dmiss_data = t5_condProbas.get_log_odds(proba_dict)\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/VESPA/vespa/predict/logodds.py:102\u001b[0m, in \u001b[0;36mT5_condProbas.get_proba_dict\u001b[0;34m(self, seq_dict, mutation_gen)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_proba_dict\u001b[39m(\u001b[39mself\u001b[39m, seq_dict, mutation_gen: utils\u001b[39m.\u001b[39mMutationGenerator):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    Compute for all residues in a protein the conditional probabilities for reconstructing single, masked tokens.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprott5\u001b[39m.\u001b[39;49mget_model(LOGODDS)\n\u001b[1;32m    104\u001b[0m     result_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m identifier, position_list \u001b[39min\u001b[39;00m tqdm(\n\u001b[1;32m    107\u001b[0m         mutation_gen\u001b[39m.\u001b[39mgenerate_mutation_mask(include_x_pos\u001b[39m=\u001b[39mREPLACE_RARE_AA),\n\u001b[1;32m    108\u001b[0m         desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtract Sequence Logodds\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m VERBOSE,\n\u001b[1;32m    110\u001b[0m     ):\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/VESPA/vespa/predict/utils_t5.py:31\u001b[0m, in \u001b[0;36mProtT5.get_model\u001b[0;34m(self, model_usage)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe intended use of ProtT5 is not implemented.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 31\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(DEVICE)\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_tokenizer()\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_vespa_marquet_from_source/lib/python3.8/site-packages/transformers/modeling_utils.py:1896\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1891\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1892\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1893\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1894\u001b[0m     )\n\u001b[1;32m   1895\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1896\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_vespa_marquet_from_source/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_vespa_marquet_from_source/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_vespa_marquet_from_source/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 641 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_vespa_marquet_from_source/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_vespa_marquet_from_source/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/projects/ashehu/akabir4/venvs/hopper_vespa_marquet_from_source/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 14.76 GiB total capacity; 9.02 GiB already allocated; 17.75 MiB free; 9.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "a_prot_seq = protid_seq_dict_list[0]\n",
    "csv_dir = home_dir+\"models/vespa_marquet/outputs/vespa/lm_outputs\"\n",
    "mutation_gen = vespa_utils.MutationGenerator(sequence_dict=a_prot_seq)\n",
    "proba_dict = t5_condProbas.get_proba_dict(a_prot_seq, mutation_gen)\n",
    "dmiss_data = t5_condProbas.get_log_odds(proba_dict)\n",
    "T5_condProbas.write_csv_dir(dmiss_data, csv_dir, mutation_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CXDA'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "seq = \"CUDA\"\n",
    "re.sub(r\"[UZOB]\", \"X\", seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vespa.predict.utils as vespa_utils\n",
    "from pathlib import Path\n",
    "seq_dict = vespa_utils.parse_fasta_input(Path(home_dir+\"models/aa_common/datasets_pmd_analysis/pmd_sequences.fasta\"))\n",
    "print(len(seq_dict))\n",
    "# protid_seq_dict_list = [{prot_id: seq} for prot_id, seq in  seq_dict.items()]\n",
    "protid_seq_dict_list = [(prot_id, seq) for prot_id, seq in  seq_dict.items()]\n",
    "protid, seq = protid_seq_dict_list[0]\n",
    "print(protid, seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopper_vespa_marquet_from_source",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
