{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "home_dir = \"../../\"\n",
    "module_path = os.path.abspath(os.path.join(home_dir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 09:08:45.034985: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-09 09:08:45.198999: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-09 09:08:45.934288: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-09 09:08:45.941475: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-09 09:08:48.789641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from proteinbert import load_pretrained_model, tokenize_seqs\n",
    "from proteinbert.tokenization import token_to_index, index_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "23\n",
      "24\n",
      "25\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(token_to_index[\"A\"])\n",
    "print(token_to_index[\"<START>\"])\n",
    "print(token_to_index[\"<END>\"])\n",
    "print(token_to_index[\"<PAD>\"])\n",
    "print(token_to_index[\"<OTHER>\"]) # non-standard amino acids are mapped tho <OTHER> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_generator, tokenizer = load_pretrained_model(local_model_dump_dir=home_dir+\"models/proteinbert_brandes/cache/\", local_model_dump_file_name=\"epoch_92400_sample_23500000.pkl\")\n",
    "model = pretrained_model_generator.create_model(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "seq = \"MGWSCLVTGAGGLLGQRIVRLLVEEKELKEIRALDKAFRPELREEFSKLQNRTKLTVLEGDILDEPFLKRACQDVSVVIHTACIIDVFGVTHRESIMNVNVKGTQLLLEACVQASVPVFIYTSSIEVAGPNSYKEIIQNGHEEEPLENTWPTPYPYSKKLAEKAVLAANGWNLKNGDTLYTCALRPTYIYGEGGPFLSASINEALNNNGILSSVGKFSTVNPVYVGNVAWAHILALRALRDPKKAPSVRGQFYYISDDTPHQSYDNLNYILSKEFGLRLDSRWSLPLTLMYWIGFLLEVVSFLLSPIYSYQPPFNRHTVTLSNSVFTFSYKKAQRDLAYKPLYSWEEAKQKTVEWVGSLVDRHKETLKSKTQ\"\n",
    "# seq = \"ABD\"\n",
    "print(len(seq))\n",
    "print(seq[254-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 26)\n",
      "21 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.967368"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer.encode_X([seq], 1024) # '<START>'=23, '<END>'=24, <PAD>=25\n",
    "logits, annotations = model(x) # shape=(n_seq=1, seq-len=1024, vocab_size=26), shape=(1, 8943)\n",
    "logits = logits[0].numpy()\n",
    "print(logits.shape)\n",
    "\n",
    "mut_pos = 254 # 1-indexed\n",
    "wt_aa,  mt_aa = \"Y\", \"D\"\n",
    "wt_tok_idx = token_to_index[\"Y\"]\n",
    "mt_tok_idx = token_to_index[\"D\"]\n",
    "print(wt_tok_idx, mt_tok_idx)\n",
    "wt_logit = logits[mut_pos][wt_tok_idx]\n",
    "mt_logit = logits[mut_pos][mt_tok_idx]\n",
    "mt_logit - wt_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(seq):\n",
    "    x = tokenizer.encode_X([seq], 1024) # '<START>'=23, '<END>'=24, <PAD>=25\n",
    "    logits, annotations = model(x) # shape=(n_seq=1, seq-len=1024, vocab_size=26), shape=(1, 8943)\n",
    "    logits = logits[0].numpy()\n",
    "    print(logits.shape)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y D\n",
      "(1024, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.89526833e-03, 5.35019790e-04, 9.69498694e-01, 1.49636180e-03,\n",
       "       1.82713103e-03, 1.99127500e-03, 1.64967985e-03, 1.35152461e-03,\n",
       "       9.04428191e-04, 2.75730365e-03, 4.63545060e-04, 3.15711368e-03,\n",
       "       9.35012999e-04, 1.15887646e-03, 1.56603870e-03, 2.60223052e-03,\n",
       "       1.51586917e-03, 1.35588011e-14, 2.09474401e-03, 5.80061576e-04,\n",
       "       6.46258400e-07, 2.01910897e-03, 3.34831503e-11, 1.37152101e-09,\n",
       "       1.01854996e-08, 9.11189457e-11], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_seq = seq\n",
    "mt_seq = list(seq)\n",
    "mt_seq[mut_pos-1] = mt_aa\n",
    "mt_seq = \"\".join(mt_seq)\n",
    "print(wt_seq[mut_pos-1], mt_seq[mut_pos-1])\n",
    "\n",
    "mt_logits = get_logits(mt_seq)\n",
    "mt_logits[mut_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.5726064e-03, 5.3479121e-04, 1.2859084e-03, 8.1308413e-04,\n",
       "       1.2502178e-03, 1.7721958e-03, 3.2831961e-03, 8.8850153e-04,\n",
       "       8.4672403e-04, 1.9653682e-03, 2.8045877e-04, 8.6312247e-03,\n",
       "       7.1712804e-04, 1.0302503e-03, 1.1809288e-03, 2.2481875e-03,\n",
       "       1.1844998e-03, 1.8173975e-15, 1.4552135e-03, 4.0520163e-04,\n",
       "       4.5650094e-07, 9.6865392e-01, 4.2524578e-12, 3.9998063e-09,\n",
       "       1.0908802e-09, 1.1904547e-10], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_logits = get_logits(wt_seq)\n",
    "wt_logits[mut_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3681573"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.linalg.norm(mt_logits[mut_pos] - wt_logits[mut_pos])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopper_proteinbert_tf_brandes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
