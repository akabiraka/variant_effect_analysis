{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_transformers_editable/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "home_dir = \"../../\"\n",
    "module_path = os.path.abspath(os.path.join(home_dir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 11.3G/11.3G [02:47<00:00, 67.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\") # this did not work\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Tokenizer(name_or_path='Rostlab/prot_t5_xl_uniref50', vocab_size=128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A S <extra_id_0> X\n",
      "{'input_ids': [[3, 7, 127, 23, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n",
      "▁A 3\n",
      "<extra_id_0> 127\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "seq = \"ASDX\"\n",
    "\n",
    "seq = re.sub(r\"[UZOB]\", \"X\", seq) # replacing unknown amino acid with unknown token\n",
    "seq = list(seq)\n",
    "\n",
    "mut_pos_zero_idxed = 2 # the outputs of mutant A_DX and AS_X are different at every positions.\n",
    "seq[mut_pos_zero_idxed] = '<extra_id_0>'# tokenizer.mask_token #'<extra_id_0>' # mut_pos must be 0-indexed. replace AA by special mask token used by the model\n",
    "\n",
    "seq = \" \".join(list(seq)) # space separated amino acids\n",
    "print(seq)\n",
    "\n",
    "# <eos> token at the end\n",
    "# starts from 0-index\n",
    "input_ids = tokenizer.batch_encode_plus(\n",
    "            [seq], add_special_tokens=True, padding=\"longest\"\n",
    "        )\n",
    "print(input_ids)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(3), tokenizer.convert_tokens_to_ids('▁A'))\n",
    "print(tokenizer.convert_ids_to_tokens(127), tokenizer.convert_tokens_to_ids('<extra_id_0>'))\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "tokenized_sequences = torch.tensor(input_ids[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(input_ids[\"attention_mask\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-17.390211   -9.293226  -50.622787   -7.236208  -10.624225   -8.695351\n",
      "  -10.065985   -9.033567   -9.414229   -9.940994  -10.468842  -10.259579\n",
      "  -12.276775   -9.225727  -11.641763  -12.024079  -11.323843  -12.420126\n",
      "  -12.841396  -12.613511  -11.722964  -12.178924  -11.511997   -4.7033205\n",
      "  -50.698643  -51.003498  -50.929955  -50.378647  -49.787445  -51.267986\n",
      "  -50.573208  -51.00089   -50.63575   -51.379807  -50.905254  -51.337032\n",
      "  -50.69402   -50.55966   -51.067028  -51.02701   -49.835644  -50.958694\n",
      "  -51.259655  -51.352947  -50.01358   -50.387264  -50.794067  -49.55082\n",
      "  -50.350464  -50.089256  -50.65592   -51.10532   -50.492172  -49.89929\n",
      "  -51.1219    -50.76748   -49.833546  -49.934505  -50.972485  -50.68562\n",
      "  -51.00729   -50.040417  -51.130943  -50.581726  -49.522335  -50.701366\n",
      "  -49.25947   -49.574875  -50.061802  -51.41051   -51.184708  -51.082794\n",
      "  -51.39099   -50.893303  -50.307026  -49.093086  -50.58528   -51.06971\n",
      "  -50.63497   -51.10927   -50.554596  -50.864094  -50.73018   -50.8275\n",
      "  -51.168915  -50.499466  -50.788994  -50.988132  -50.51436   -50.605606\n",
      "  -50.260715  -51.41411   -50.224106  -51.392494  -49.90922   -49.23191\n",
      "  -51.41794   -51.181614  -50.328217  -51.14015   -49.874557  -50.672993\n",
      "  -50.923485  -49.79606   -50.190014  -50.252045  -50.85577   -49.524544\n",
      "  -50.6327    -51.26859   -50.34061   -50.968613  -49.79003   -50.78691\n",
      "  -49.494213  -50.34453   -50.288982  -50.546944  -50.714638  -50.081295\n",
      "  -51.414192  -49.42196   -50.91297   -50.583344  -50.581802  -50.20838\n",
      "  -50.64206   -21.140263 ]\n",
      " [-16.866272   -8.320622  -49.29569    -7.9538    -10.331341   -8.666181\n",
      "   -9.859677   -8.588242   -9.387717   -9.76171   -10.417749  -10.051477\n",
      "  -11.492709   -9.098804  -11.175884  -11.22582   -11.122851  -11.785486\n",
      "  -12.242562  -12.336056  -11.452854  -11.943663  -11.164154   -4.102235\n",
      "  -49.345673  -49.688103  -49.586235  -49.040653  -48.45044   -49.968243\n",
      "  -49.233498  -49.671555  -49.320175  -50.06217   -49.599796  -49.99944\n",
      "  -49.354416  -49.224678  -49.737602  -49.6809    -48.530663  -49.64503\n",
      "  -49.860508  -50.01845   -48.68847   -49.016853  -49.467598  -48.219204\n",
      "  -49.07787   -48.72699   -49.308563  -49.71879   -49.165432  -48.5612\n",
      "  -49.76278   -49.41576   -48.46483   -48.58905   -49.62636   -49.351795\n",
      "  -49.60171   -48.7232    -49.786186  -49.25646   -48.22087   -49.36521\n",
      "  -47.941494  -48.263695  -48.741814  -50.08765   -49.796364  -49.76068\n",
      "  -50.04667   -49.60022   -48.954624  -47.7401    -49.229164  -49.775677\n",
      "  -49.280743  -49.774574  -49.261528  -49.562263  -49.408504  -49.507576\n",
      "  -49.804367  -49.141605  -49.470325  -49.610428  -49.180363  -49.283737\n",
      "  -48.92518   -50.04693   -48.874874  -50.005127  -48.568047  -47.955685\n",
      "  -50.07725   -49.825058  -48.970276  -49.815056  -48.574497  -49.330097\n",
      "  -49.575874  -48.510273  -48.81609   -48.9332    -49.471664  -48.17591\n",
      "  -49.293076  -49.93109   -48.999348  -49.633266  -48.44925   -49.483315\n",
      "  -48.164864  -49.062332  -48.94882   -49.230434  -49.373257  -48.738747\n",
      "  -50.06377   -48.05665   -49.52244   -49.287983  -49.240227  -48.91617\n",
      "  -49.324657  -20.704407 ]\n",
      " [-17.898945   -9.202403  -53.122887   -9.574697  -12.7445755 -10.559284\n",
      "  -12.098485  -10.529473  -11.33949   -11.459851  -11.168965  -11.437575\n",
      "  -13.042727  -10.80097   -12.375878  -13.169664  -12.870213  -12.5402775\n",
      "  -13.58986   -14.120016  -12.793388  -13.800516  -13.097416   -5.6513147\n",
      "  -53.21032   -53.579628  -53.431843  -52.84589   -52.178284  -53.898567\n",
      "  -53.02117   -53.507946  -53.200832  -53.89363   -53.488167  -53.9172\n",
      "  -53.2298    -53.058975  -53.61648   -53.630035  -52.30913   -53.609264\n",
      "  -53.69078   -53.927162  -52.4782    -52.898754  -53.307434  -51.985046\n",
      "  -52.82623   -52.599876  -53.137394  -53.639782  -52.92334   -52.265854\n",
      "  -53.694115  -53.248375  -52.117096  -52.32519   -53.5383    -53.21218\n",
      "  -53.45483   -52.47364   -53.6539    -53.14419   -51.923103  -53.1452\n",
      "  -51.63178   -52.06701   -52.544205  -54.008762  -53.655563  -53.66865\n",
      "  -54.031837  -53.472584  -52.715096  -51.358433  -53.108204  -53.628654\n",
      "  -53.1255    -53.605515  -53.184532  -53.453148  -53.33865   -53.453335\n",
      "  -53.76884   -52.99042   -53.340515  -53.47511   -53.063404  -53.124737\n",
      "  -52.71729   -54.00177   -52.69354   -53.80423   -52.3895    -51.682354\n",
      "  -54.06133   -53.781487  -52.897736  -53.79196   -52.391838  -53.19596\n",
      "  -53.43569   -52.24811   -52.667007  -52.66067   -53.35977   -51.81193\n",
      "  -53.234478  -53.79413   -52.82883   -53.550484  -52.153797  -53.437176\n",
      "  -51.922276  -52.882652  -52.76003   -52.944717  -53.26416   -52.442135\n",
      "  -54.014915  -51.800575  -53.346188  -53.208492  -53.00592   -52.81154\n",
      "  -53.13237   -23.477278 ]\n",
      " [-20.737772   -3.7584524 -57.735935  -11.91381   -14.365265  -13.628492\n",
      "  -14.916158  -12.088224  -15.53883   -13.017007  -14.2609625 -14.676021\n",
      "  -16.152258  -13.286839  -14.8951645 -15.14369   -13.879182  -15.817013\n",
      "  -15.066701  -15.652651  -16.122252  -15.847937  -16.030632  -11.647527\n",
      "  -57.915493  -58.27983   -58.00041   -57.39017   -56.780178  -58.61351\n",
      "  -57.70802   -58.146854  -57.854084  -58.68312   -58.083702  -58.52318\n",
      "  -57.898365  -57.66031   -58.23515   -58.23633   -56.775887  -58.260742\n",
      "  -58.20879   -58.67041   -57.00826   -57.528904  -57.810684  -56.47801\n",
      "  -57.542656  -57.1828    -57.70937   -58.172653  -57.552353  -56.94148\n",
      "  -58.40102   -57.868977  -56.68635   -56.79447   -58.134113  -58.030785\n",
      "  -58.06502   -56.973675  -58.28252   -57.84807   -56.36638   -57.781803\n",
      "  -56.129925  -56.523067  -57.16838   -58.635475  -58.331932  -58.38269\n",
      "  -58.74457   -58.131054  -57.237827  -55.66375   -57.76955   -58.299194\n",
      "  -57.47014   -58.23658   -57.828857  -57.932026  -57.725727  -57.9335\n",
      "  -58.317486  -57.55193   -58.025475  -58.12502   -57.699352  -57.853104\n",
      "  -57.174     -58.672646  -57.17393   -58.435196  -57.027866  -56.209038\n",
      "  -58.66261   -58.408146  -57.61858   -58.36116   -56.9629    -57.767063\n",
      "  -57.96803   -56.670914  -57.26626   -57.172825  -58.071266  -56.409584\n",
      "  -57.80252   -58.549095  -57.33623   -58.164925  -56.64548   -57.963055\n",
      "  -56.54416   -57.367226  -57.36033   -57.660095  -57.803684  -56.968628\n",
      "  -58.724075  -56.37972   -57.84558   -57.874073  -57.75901   -57.418888\n",
      "  -57.624893  -29.16351  ]]\n",
      "(4, 128)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input_ids=tokenized_sequences, attention_mask=attention_mask, decoder_input_ids=tokenized_sequences).logits\n",
    "\n",
    "logits = logits.squeeze().cpu().numpy()\n",
    "logits = logits[0:4]\n",
    "print(logits)\n",
    "print(logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopper_transformers_editable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
