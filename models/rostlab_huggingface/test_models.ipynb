{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/ashehu/akabir4/venvs/hopper_transformers_editable/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "home_dir = \"../../\"\n",
    "module_path = os.path.abspath(os.path.join(home_dir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 9.49kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 546/546 [00:00<00:00, 262kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 238k/238k [00:00<00:00, 15.0MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 824kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 8.77kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5Tokenizer(name_or_path='Rostlab/prot_t5_xl_uniref50', vocab_size=128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", use_fast=False, force_download=True) # this did not work\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[127,   7,  10,  23,   1]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "seq = \"ASDX\"\n",
    "seq = re.sub(r\"[UZOB]\", \"X\", seq) # replacing unknown amino acids \n",
    "seq = list(seq)\n",
    "seq[0] = \"<extra_id_0>\" # mut_pos should be 0-indexed. replace AA by special mask token used by T5\n",
    "seq = \" \".join(seq)\n",
    "input_ids = tokenizer(seq, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.9623642e+00, -1.1519006e+01, -2.6319691e+01, -2.1865085e-01,\n",
       "         2.9277468e-01, -1.0201454e-02, -2.7182025e-01,  5.2822196e-01,\n",
       "        -2.8677055e-01,  1.6395064e-01,  2.4561551e-01, -3.6883721e-01,\n",
       "        -3.4541780e-01, -3.1883174e-01, -2.5528312e-01, -5.1485687e-01,\n",
       "        -7.5694716e-01, -5.3059280e-01, -1.0208641e+00, -1.0341456e+00,\n",
       "        -1.2978854e+00, -1.5801252e+00, -1.6105332e+00, -9.6087420e-01,\n",
       "        -2.6679211e+01, -2.6762032e+01, -2.6532936e+01, -2.6364166e+01,\n",
       "        -2.5991905e+01, -2.6679920e+01, -2.6217539e+01, -2.6640343e+01,\n",
       "        -2.6261005e+01, -2.6664814e+01, -2.6575035e+01, -2.6799864e+01,\n",
       "        -2.6431791e+01, -2.6238785e+01, -2.6557615e+01, -2.6683323e+01,\n",
       "        -2.5811712e+01, -2.6584278e+01, -2.6596745e+01, -2.6556015e+01,\n",
       "        -2.5744869e+01, -2.6089424e+01, -2.6392120e+01, -2.5533455e+01,\n",
       "        -2.6202869e+01, -2.6128756e+01, -2.6233582e+01, -2.6652227e+01,\n",
       "        -2.6194559e+01, -2.5723972e+01, -2.6535376e+01, -2.6237965e+01,\n",
       "        -2.5925362e+01, -2.6076824e+01, -2.6585743e+01, -2.6239929e+01,\n",
       "        -2.6294964e+01, -2.5908783e+01, -2.6486706e+01, -2.6222719e+01,\n",
       "        -2.5744759e+01, -2.6504322e+01, -2.5615446e+01, -2.5875483e+01,\n",
       "        -2.6188038e+01, -2.6648571e+01, -2.6522141e+01, -2.6757744e+01,\n",
       "        -2.6779964e+01, -2.6584366e+01, -2.6103792e+01, -2.5533970e+01,\n",
       "        -2.6360161e+01, -2.6710659e+01, -2.6250429e+01, -2.6492170e+01,\n",
       "        -2.6457176e+01, -2.6489321e+01, -2.6267778e+01, -2.6547813e+01,\n",
       "        -2.6452728e+01, -2.6063169e+01, -2.6300097e+01, -2.6449738e+01,\n",
       "        -2.6294737e+01, -2.6392666e+01, -2.5895864e+01, -2.6542124e+01,\n",
       "        -2.6242245e+01, -2.6697828e+01, -2.5939047e+01, -2.5675564e+01,\n",
       "        -2.6709457e+01, -2.6532314e+01, -2.6311062e+01, -2.6705853e+01,\n",
       "        -2.6020588e+01, -2.6074682e+01, -2.6312977e+01, -2.5943188e+01,\n",
       "        -2.6073101e+01, -2.6141977e+01, -2.6411728e+01, -2.5666138e+01,\n",
       "        -2.6356499e+01, -2.6606491e+01, -2.6265202e+01, -2.6664242e+01,\n",
       "        -2.6126049e+01, -2.6318369e+01, -2.5779884e+01, -2.6166471e+01,\n",
       "        -2.5772909e+01, -2.6316513e+01, -2.6335405e+01, -2.6002726e+01,\n",
       "        -2.6659367e+01, -2.5666023e+01, -2.6532026e+01, -2.6435795e+01,\n",
       "        -2.6523766e+01, -2.6190720e+01, -2.6236872e+01, -8.6522446e+00],\n",
       "       [-3.2389935e+01, -4.7174438e+01, -9.2213356e+01, -2.0749582e+01,\n",
       "        -2.1469732e+01, -2.0240543e+01, -2.2311827e+01, -4.2497425e+00,\n",
       "        -2.1540211e+01, -2.0150417e+01, -2.0260822e+01, -2.0901390e+01,\n",
       "        -2.2487774e+01, -2.0603516e+01, -2.1492783e+01, -1.9869171e+01,\n",
       "        -2.2260225e+01, -2.0721945e+01, -2.1856743e+01, -2.3312614e+01,\n",
       "        -2.2537373e+01, -2.3621944e+01, -2.1328402e+01, -1.9299738e+01,\n",
       "        -9.3237793e+01, -9.3312576e+01, -9.3037918e+01, -9.2902122e+01,\n",
       "        -9.2344734e+01, -9.3582031e+01, -9.3089668e+01, -9.2940727e+01,\n",
       "        -9.2667496e+01, -9.3778008e+01, -9.3628357e+01, -9.3874954e+01,\n",
       "        -9.3404488e+01, -9.2654099e+01, -9.3430161e+01, -9.3534088e+01,\n",
       "        -9.2046906e+01, -9.3697166e+01, -9.3678337e+01, -9.3919662e+01,\n",
       "        -9.2981873e+01, -9.3516785e+01, -9.3411346e+01, -9.2309555e+01,\n",
       "        -9.2633316e+01, -9.3050911e+01, -9.3196213e+01, -9.3195198e+01,\n",
       "        -9.2764977e+01, -9.2624100e+01, -9.3890770e+01, -9.3529930e+01,\n",
       "        -9.1293274e+01, -9.2018929e+01, -9.4017517e+01, -9.2877464e+01,\n",
       "        -9.3077919e+01, -9.3599792e+01, -9.3642487e+01, -9.2916946e+01,\n",
       "        -9.1567619e+01, -9.3032951e+01, -9.1841156e+01, -9.2503410e+01,\n",
       "        -9.2426216e+01, -9.4834366e+01, -9.3713997e+01, -9.3312317e+01,\n",
       "        -9.3086082e+01, -9.2982315e+01, -9.2311508e+01, -9.1979126e+01,\n",
       "        -9.3190369e+01, -9.3269226e+01, -9.3240463e+01, -9.3786583e+01,\n",
       "        -9.2914398e+01, -9.2953850e+01, -9.3054977e+01, -9.3694336e+01,\n",
       "        -9.4364532e+01, -9.3277245e+01, -9.3430435e+01, -9.3589401e+01,\n",
       "        -9.2765633e+01, -9.3376495e+01, -9.2914429e+01, -9.3993271e+01,\n",
       "        -9.2298676e+01, -9.3430878e+01, -9.2442047e+01, -9.1838875e+01,\n",
       "        -9.3961960e+01, -9.3777161e+01, -9.2937164e+01, -9.3858414e+01,\n",
       "        -9.2170891e+01, -9.3687866e+01, -9.3342621e+01, -9.2024979e+01,\n",
       "        -9.2737129e+01, -9.2916626e+01, -9.2879150e+01, -9.2077286e+01,\n",
       "        -9.3420952e+01, -9.3431396e+01, -9.2191719e+01, -9.2467773e+01,\n",
       "        -9.2169868e+01, -9.3331017e+01, -9.2991463e+01, -9.2822487e+01,\n",
       "        -9.2543190e+01, -9.2576988e+01, -9.3216721e+01, -9.2606415e+01,\n",
       "        -9.3748810e+01, -9.2141457e+01, -9.3206589e+01, -9.3182190e+01,\n",
       "        -9.2861008e+01, -9.2374893e+01, -9.2688553e+01, -3.9184578e+01],\n",
       "       [-2.8973919e+01, -4.0262676e+01, -6.3432098e+01, -2.1038414e+01,\n",
       "        -2.4317722e+01, -2.1278889e+01, -2.3207993e+01, -2.2139326e+01,\n",
       "        -2.4560755e+01, -1.9727505e+01, -1.3622727e+00, -2.2209766e+01,\n",
       "        -2.4904291e+01, -2.1859417e+01, -2.4180889e+01, -2.5048252e+01,\n",
       "        -2.4614214e+01, -2.1565870e+01, -2.3634586e+01, -2.6737621e+01,\n",
       "        -2.3169329e+01, -2.5906799e+01, -2.6040203e+01, -2.0939156e+01,\n",
       "        -6.4247742e+01, -6.4243629e+01, -6.3722263e+01, -6.3677139e+01,\n",
       "        -6.3392563e+01, -6.4130554e+01, -6.3703316e+01, -6.4177505e+01,\n",
       "        -6.3651402e+01, -6.4627869e+01, -6.4007805e+01, -6.4470360e+01,\n",
       "        -6.4097862e+01, -6.3279602e+01, -6.4226913e+01, -6.4256302e+01,\n",
       "        -6.3087612e+01, -6.4403275e+01, -6.4250565e+01, -6.4288116e+01,\n",
       "        -6.2994755e+01, -6.3654716e+01, -6.3529877e+01, -6.3164013e+01,\n",
       "        -6.3239197e+01, -6.3750370e+01, -6.3950840e+01, -6.4367584e+01,\n",
       "        -6.3378983e+01, -6.3249680e+01, -6.4454727e+01, -6.3972401e+01,\n",
       "        -6.2419495e+01, -6.2969891e+01, -6.4079712e+01, -6.3527775e+01,\n",
       "        -6.3834560e+01, -6.3620346e+01, -6.3652851e+01, -6.3326420e+01,\n",
       "        -6.2496429e+01, -6.3674957e+01, -6.2679581e+01, -6.3409870e+01,\n",
       "        -6.3177551e+01, -6.4525314e+01, -6.3865677e+01, -6.4002708e+01,\n",
       "        -6.4103462e+01, -6.3696014e+01, -6.3126595e+01, -6.2605026e+01,\n",
       "        -6.3658619e+01, -6.3726433e+01, -6.3440655e+01, -6.4362694e+01,\n",
       "        -6.3667747e+01, -6.3843834e+01, -6.3837578e+01, -6.3986656e+01,\n",
       "        -6.4361397e+01, -6.3481915e+01, -6.3827133e+01, -6.4201309e+01,\n",
       "        -6.3443481e+01, -6.3537041e+01, -6.3163776e+01, -6.4734695e+01,\n",
       "        -6.3614357e+01, -6.3900703e+01, -6.3169636e+01, -6.2538567e+01,\n",
       "        -6.4484116e+01, -6.4295837e+01, -6.3861519e+01, -6.4900673e+01,\n",
       "        -6.3139778e+01, -6.3946629e+01, -6.4211266e+01, -6.2805565e+01,\n",
       "        -6.3459831e+01, -6.3265690e+01, -6.3918613e+01, -6.2563385e+01,\n",
       "        -6.4170311e+01, -6.4151413e+01, -6.3215706e+01, -6.3732452e+01,\n",
       "        -6.3037235e+01, -6.3838108e+01, -6.2903893e+01, -6.3319351e+01,\n",
       "        -6.3183952e+01, -6.3556164e+01, -6.4264954e+01, -6.3279900e+01,\n",
       "        -6.4550842e+01, -6.3205868e+01, -6.3932621e+01, -6.3562969e+01,\n",
       "        -6.3792316e+01, -6.3165077e+01, -6.3242760e+01, -4.1839394e+01],\n",
       "       [-2.4530979e+01, -3.1375896e+01, -6.1742184e+01, -2.6498724e+01,\n",
       "        -2.6919512e+01, -2.3313786e+01, -2.2038284e+01, -2.2694887e+01,\n",
       "        -2.1053316e+01, -2.3186836e+01, -2.1389015e+01, -2.1536171e+01,\n",
       "        -1.9836876e+01, -2.1927052e+01, -1.9870907e+01, -2.0883514e+01,\n",
       "        -2.9101414e+01, -2.1896965e+01, -2.3818110e+01, -2.7591169e+01,\n",
       "        -2.3616676e+01, -2.4434532e+01, -2.2869755e+01,  4.4548464e+00,\n",
       "        -6.2162003e+01, -6.2734695e+01, -6.2497639e+01, -6.1815422e+01,\n",
       "        -6.0659542e+01, -6.2702496e+01, -6.1665146e+01, -6.1965809e+01,\n",
       "        -6.2161674e+01, -6.2742458e+01, -6.2421795e+01, -6.2849777e+01,\n",
       "        -6.1706329e+01, -6.2021954e+01, -6.2529984e+01, -6.2636909e+01,\n",
       "        -6.1093365e+01, -6.2798157e+01, -6.2325207e+01, -6.2797031e+01,\n",
       "        -6.1053391e+01, -6.1386772e+01, -6.2067467e+01, -6.0511555e+01,\n",
       "        -6.1358383e+01, -6.1462814e+01, -6.1936016e+01, -6.2652046e+01,\n",
       "        -6.1697845e+01, -6.0686493e+01, -6.2637730e+01, -6.1728947e+01,\n",
       "        -6.0849144e+01, -6.0985771e+01, -6.2808281e+01, -6.1866722e+01,\n",
       "        -6.2209244e+01, -6.1289009e+01, -6.2346321e+01, -6.1362144e+01,\n",
       "        -6.0728378e+01, -6.1568939e+01, -6.0104294e+01, -6.0673302e+01,\n",
       "        -6.1168259e+01, -6.3225121e+01, -6.2487103e+01, -6.2058716e+01,\n",
       "        -6.2891037e+01, -6.2053116e+01, -6.1422211e+01, -6.0074722e+01,\n",
       "        -6.1952194e+01, -6.2615242e+01, -6.1845623e+01, -6.2563671e+01,\n",
       "        -6.1839596e+01, -6.2125046e+01, -6.2191685e+01, -6.2153385e+01,\n",
       "        -6.2649048e+01, -6.1790382e+01, -6.2157459e+01, -6.2368656e+01,\n",
       "        -6.1905396e+01, -6.1872330e+01, -6.1585445e+01, -6.3049793e+01,\n",
       "        -6.1320789e+01, -6.2836983e+01, -6.0637394e+01, -6.0263920e+01,\n",
       "        -6.2934425e+01, -6.2521038e+01, -6.1560425e+01, -6.2825062e+01,\n",
       "        -6.1239330e+01, -6.2013367e+01, -6.2317226e+01, -6.0976265e+01,\n",
       "        -6.1502968e+01, -6.1371773e+01, -6.2247948e+01, -6.0406185e+01,\n",
       "        -6.1962139e+01, -6.2484859e+01, -6.1593819e+01, -6.2246658e+01,\n",
       "        -6.0769554e+01, -6.2330769e+01, -6.0447159e+01, -6.1627781e+01,\n",
       "        -6.1505180e+01, -6.1474930e+01, -6.2173832e+01, -6.1105732e+01,\n",
       "        -6.2720768e+01, -6.0230007e+01, -6.2162155e+01, -6.1812752e+01,\n",
       "        -6.1528641e+01, -6.1502140e+01, -6.1966072e+01, -3.4690300e+01],\n",
       "       [-2.4273060e+01,  2.3213787e+00, -5.6778564e+01, -2.1173382e+01,\n",
       "        -2.1862240e+01, -1.9884047e+01, -2.2938641e+01, -2.0370371e+01,\n",
       "        -1.9789703e+01, -1.9026077e+01, -2.0757885e+01, -2.2362932e+01,\n",
       "        -2.3588312e+01, -1.9095951e+01, -1.8519690e+01, -2.0687107e+01,\n",
       "        -2.0937601e+01, -2.2103773e+01, -2.1499563e+01, -2.2446709e+01,\n",
       "        -2.2415272e+01, -2.0472296e+01, -2.1914328e+01, -1.4122541e+01,\n",
       "        -5.6957237e+01, -5.7418228e+01, -5.7102757e+01, -5.6493042e+01,\n",
       "        -5.5585342e+01, -5.7605179e+01, -5.6821571e+01, -5.7103558e+01,\n",
       "        -5.6990158e+01, -5.7708847e+01, -5.7148247e+01, -5.7431828e+01,\n",
       "        -5.6604103e+01, -5.6698650e+01, -5.7260342e+01, -5.7196426e+01,\n",
       "        -5.5822060e+01, -5.7126869e+01, -5.7096340e+01, -5.7781998e+01,\n",
       "        -5.5796497e+01, -5.6406612e+01, -5.6593399e+01, -5.5420017e+01,\n",
       "        -5.6662422e+01, -5.6383049e+01, -5.6639297e+01, -5.7167862e+01,\n",
       "        -5.6559128e+01, -5.5902580e+01, -5.7450912e+01, -5.6676453e+01,\n",
       "        -5.5728302e+01, -5.5717796e+01, -5.7111488e+01, -5.7238380e+01,\n",
       "        -5.7053726e+01, -5.5831383e+01, -5.7193489e+01, -5.6832077e+01,\n",
       "        -5.5341759e+01, -5.6669704e+01, -5.5050732e+01, -5.5483891e+01,\n",
       "        -5.6277027e+01, -5.7725483e+01, -5.7450546e+01, -5.7472046e+01,\n",
       "        -5.7815399e+01, -5.7130333e+01, -5.6080223e+01, -5.4716949e+01,\n",
       "        -5.6844372e+01, -5.7452324e+01, -5.6341217e+01, -5.7468521e+01,\n",
       "        -5.6834602e+01, -5.6959641e+01, -5.6614021e+01, -5.6743114e+01,\n",
       "        -5.7004562e+01, -5.6429272e+01, -5.7063496e+01, -5.7019142e+01,\n",
       "        -5.6646160e+01, -5.6900921e+01, -5.5977348e+01, -5.7672676e+01,\n",
       "        -5.5998894e+01, -5.7449684e+01, -5.5939079e+01, -5.5151955e+01,\n",
       "        -5.7553967e+01, -5.7269379e+01, -5.6475910e+01, -5.7310738e+01,\n",
       "        -5.6067997e+01, -5.6710091e+01, -5.6733116e+01, -5.5626999e+01,\n",
       "        -5.6130836e+01, -5.6004311e+01, -5.7158302e+01, -5.5471191e+01,\n",
       "        -5.6702316e+01, -5.7670952e+01, -5.6326134e+01, -5.7089752e+01,\n",
       "        -5.5653671e+01, -5.6896835e+01, -5.5698517e+01, -5.6271305e+01,\n",
       "        -5.6325180e+01, -5.6833519e+01, -5.6776772e+01, -5.5984421e+01,\n",
       "        -5.7639847e+01, -5.5400402e+01, -5.6731102e+01, -5.6727264e+01,\n",
       "        -5.6826607e+01, -5.6340252e+01, -5.6603844e+01, -3.9863007e+01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, labels=input_ids).logits\n",
    "logits = logits.squeeze().detach().numpy() \n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A S <extra_id_0> X\n",
      "{'input_ids': [[3, 7, 127, 23, 1]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n",
      "▁A 3\n",
      "<extra_id_0> 127\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "seq = \"ASDX\"\n",
    "\n",
    "seq = re.sub(r\"[UZOB]\", \"X\", seq) # replacing unknown amino acid with unknown token\n",
    "seq = list(seq)\n",
    "\n",
    "mut_pos_zero_idxed = 2 # the outputs of mutant A_DX and AS_X are different at every positions.\n",
    "seq[mut_pos_zero_idxed] = '<extra_id_0>'# tokenizer.mask_token #'<extra_id_0>' # mut_pos must be 0-indexed. replace AA by special mask token used by the model\n",
    "\n",
    "seq = \" \".join(list(seq)) # space separated amino acids\n",
    "print(seq)\n",
    "\n",
    "# <eos> token at the end\n",
    "# starts from 0-index\n",
    "input_ids = tokenizer.batch_encode_plus(\n",
    "            [seq], add_special_tokens=True, padding=\"longest\"\n",
    "        )\n",
    "print(input_ids)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(3), tokenizer.convert_tokens_to_ids('▁A'))\n",
    "print(tokenizer.convert_ids_to_tokens(127), tokenizer.convert_tokens_to_ids('<extra_id_0>'))\n",
    "\n",
    "import torch\n",
    "tokenized_sequences = torch.tensor(input_ids[\"input_ids\"]).to(\"cpu\")\n",
    "attention_mask = torch.tensor(input_ids[\"attention_mask\"]).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-17.390213   -9.293228  -50.622787   -7.236208  -10.624226   -8.6953535\n",
      "  -10.065987   -9.033568   -9.414228   -9.940994  -10.468842  -10.259581\n",
      "  -12.276777   -9.225728  -11.641765  -12.024082  -11.323845  -12.420128\n",
      "  -12.841398  -12.613513  -11.722965  -12.178925  -11.511997   -4.7033224\n",
      "  -50.698647  -51.0035    -50.929955  -50.378654  -49.78744   -51.267982\n",
      "  -50.573204  -51.000893  -50.63575   -51.379814  -50.905254  -51.337036\n",
      "  -50.694023  -50.559666  -51.067024  -51.027008  -49.835648  -50.958687\n",
      "  -51.259655  -51.352947  -50.01358   -50.387268  -50.794064  -49.550823\n",
      "  -50.35047   -50.089256  -50.655922  -51.105324  -50.492165  -49.899284\n",
      "  -51.121902  -50.767487  -49.83354   -49.934505  -50.972477  -50.685616\n",
      "  -51.007286  -50.040413  -51.130943  -50.581734  -49.522335  -50.701363\n",
      "  -49.25947   -49.57488   -50.061813  -51.410515  -51.1847    -51.082794\n",
      "  -51.39098   -50.893303  -50.307026  -49.09309   -50.58528   -51.06971\n",
      "  -50.63497   -51.109264  -50.554596  -50.864098  -50.73017   -50.827507\n",
      "  -51.168922  -50.499466  -50.78899   -50.988136  -50.51435   -50.605606\n",
      "  -50.26072   -51.4141    -50.224106  -51.392494  -49.909225  -49.23191\n",
      "  -51.41794   -51.181618  -50.32821   -51.140144  -49.87456   -50.67299\n",
      "  -50.923485  -49.79606   -50.190014  -50.252045  -50.85577   -49.524536\n",
      "  -50.632706  -51.268593  -50.340614  -50.968613  -49.79003   -50.78691\n",
      "  -49.494217  -50.34453   -50.288986  -50.546944  -50.714645  -50.0813\n",
      "  -51.414192  -49.42196   -50.91297   -50.583344  -50.58181   -50.20838\n",
      "  -50.642063  -21.140263 ]\n",
      " [-16.866276   -8.320623  -49.295692   -7.953805  -10.3313465  -8.666183\n",
      "   -9.859682   -8.588247   -9.387722   -9.761713  -10.417755  -10.051483\n",
      "  -11.492715   -9.098809  -11.175888  -11.225824  -11.122856  -11.785491\n",
      "  -12.242565  -12.3360615 -11.45286   -11.9436655 -11.164158   -4.102238\n",
      "  -49.34568   -49.68811   -49.586243  -49.04065   -48.45044   -49.968246\n",
      "  -49.233498  -49.671566  -49.32018   -50.062172  -49.599804  -49.999447\n",
      "  -49.354424  -49.22468   -49.737602  -49.68091   -48.530663  -49.64504\n",
      "  -49.86051   -50.01846   -48.68847   -49.01685   -49.467606  -48.2192\n",
      "  -49.077873  -48.726994  -49.30858   -49.718796  -49.16543   -48.561203\n",
      "  -49.76278   -49.415768  -48.46484   -48.589054  -49.62636   -49.351807\n",
      "  -49.601715  -48.7232    -49.78619   -49.25646   -48.220882  -49.36522\n",
      "  -47.941505  -48.263702  -48.741825  -50.087654  -49.796364  -49.760685\n",
      "  -50.046677  -49.600227  -48.954624  -47.740112  -49.22917   -49.77568\n",
      "  -49.280743  -49.774586  -49.261536  -49.562267  -49.408512  -49.507584\n",
      "  -49.804375  -49.141617  -49.47033   -49.610435  -49.18036   -49.283745\n",
      "  -48.925175  -50.046932  -48.874878  -50.005135  -48.56805   -47.955696\n",
      "  -50.07726   -49.82505   -48.970276  -49.81506   -48.5745    -49.33011\n",
      "  -49.575874  -48.510273  -48.816093  -48.933216  -49.471672  -48.17591\n",
      "  -49.293076  -49.931103  -48.99935   -49.633266  -48.44925   -49.483322\n",
      "  -48.16487   -49.062336  -48.948822  -49.23044   -49.373257  -48.73875\n",
      "  -50.063778  -48.056656  -49.52245   -49.28798   -49.240234  -48.916176\n",
      "  -49.324654  -20.70441  ]\n",
      " [-17.898949   -9.202409  -53.122902   -9.574698  -12.744577  -10.559287\n",
      "  -12.098488  -10.529477  -11.339495  -11.459857  -11.168972  -11.437581\n",
      "  -13.042729  -10.800976  -12.375883  -13.169668  -12.87022   -12.540282\n",
      "  -13.589865  -14.120024  -12.793392  -13.800518  -13.097418   -5.651314\n",
      "  -53.210327  -53.579636  -53.431854  -52.845898  -52.17829   -53.898582\n",
      "  -53.021175  -53.507957  -53.200844  -53.893642  -53.488167  -53.917213\n",
      "  -53.22981   -53.05898   -53.616493  -53.630043  -52.309143  -53.609276\n",
      "  -53.690792  -53.927177  -52.47821   -52.898766  -53.30744   -51.985054\n",
      "  -52.826233  -52.599888  -53.137398  -53.639786  -52.923347  -52.265865\n",
      "  -53.69413   -53.248383  -52.117107  -52.325203  -53.538315  -53.212196\n",
      "  -53.45484   -52.473656  -53.653908  -53.1442    -51.923122  -53.145203\n",
      "  -51.63179   -52.067028  -52.544224  -54.008774  -53.65557   -53.66866\n",
      "  -54.031853  -53.4726    -52.71511   -51.35844   -53.10821   -53.628662\n",
      "  -53.125507  -53.605522  -53.18454   -53.45316   -53.338657  -53.45335\n",
      "  -53.76886   -52.990437  -53.340527  -53.47512   -53.06342   -53.12475\n",
      "  -52.717297  -54.001785  -52.693546  -53.804245  -52.38951   -51.68236\n",
      "  -54.06134   -53.7815    -52.89775   -53.791973  -52.39185   -53.19597\n",
      "  -53.435707  -52.248116  -52.667015  -52.660675  -53.35978   -51.811943\n",
      "  -53.23449   -53.794136  -52.82884   -53.550495  -52.15381   -53.437187\n",
      "  -51.922287  -52.882668  -52.76004   -52.94473   -53.264168  -52.44215\n",
      "  -54.014923  -51.80059   -53.3462    -53.208504  -53.005928  -52.811546\n",
      "  -53.13238   -23.477287 ]\n",
      " [-20.737776   -3.75845   -57.73592   -11.913813  -14.365272  -13.628497\n",
      "  -14.916163  -12.088228  -15.538836  -13.017012  -14.260968  -14.676027\n",
      "  -16.152266  -13.286842  -14.89517   -15.143695  -13.879187  -15.8170185\n",
      "  -15.066705  -15.652655  -16.122255  -15.847941  -16.030636  -11.647531\n",
      "  -57.9155    -58.27983   -58.000412  -57.390175  -56.78018   -58.613518\n",
      "  -57.70803   -58.146854  -57.854088  -58.683132  -58.08371   -58.523186\n",
      "  -57.89837   -57.66031   -58.23515   -58.236324  -56.775887  -58.26075\n",
      "  -58.208794  -58.670418  -57.00827   -57.5289    -57.81069   -56.478012\n",
      "  -57.542656  -57.1828    -57.70939   -58.172653  -57.552357  -56.941483\n",
      "  -58.401024  -57.86898   -56.68636   -56.794483  -58.134125  -58.030804\n",
      "  -58.065033  -56.973686  -58.28252   -57.848057  -56.36638   -57.78181\n",
      "  -56.12993   -56.52307   -57.168392  -58.635468  -58.331944  -58.382698\n",
      "  -58.744576  -58.13106   -57.237835  -55.66375   -57.769547  -58.2992\n",
      "  -57.470146  -58.236584  -57.828857  -57.932034  -57.72573   -57.93351\n",
      "  -58.317497  -57.551933  -58.025482  -58.12502   -57.69935   -57.853104\n",
      "  -57.17399   -58.672646  -57.173943  -58.435196  -57.027866  -56.20904\n",
      "  -58.662613  -58.408157  -57.618587  -58.361164  -56.962906  -57.767075\n",
      "  -57.968033  -56.670906  -57.26626   -57.172836  -58.071266  -56.40959\n",
      "  -57.802525  -58.54911   -57.336235  -58.164932  -56.6455    -57.96305\n",
      "  -56.544167  -57.367226  -57.360332  -57.660103  -57.803696  -56.968636\n",
      "  -58.724075  -56.379723  -57.845592  -57.874077  -57.759007  -57.418888\n",
      "  -57.624897  -29.163519 ]]\n",
      "(4, 128)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input_ids=tokenized_sequences, attention_mask=attention_mask, decoder_input_ids=tokenized_sequences).logits\n",
    "\n",
    "logits = logits.squeeze().cpu().numpy()\n",
    "logits = logits[0:4]\n",
    "print(logits)\n",
    "print(logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopper_transformers_editable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
